{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSeUL0IdXN96"
   },
   "source": [
    "### Set up google colab and unzip train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UTs4M8P2XDnm"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9zRxDGEsfi3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import math\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import ops\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image,ImageFilter, ImageEnhance\n",
    "import cv2\n",
    "import torch.optim as optim\n",
    "from torchvision.ops import nms\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bfM1sOVGWTVt"
   },
   "source": [
    "### Define Model Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JUeu8zmRRj2d",
    "outputId": "ddb57cfd-8353-4494-e916-ef3e215fe56e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "select_classes = {'aeroplane', 'bicycle','boat','bus', 'dog','train','motorbike'} # Only training for these classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VDwEFK8tcolt"
   },
   "source": [
    "### Handle Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rqgc_5G3XYXx"
   },
   "source": [
    "##### Get all classes and create label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "6tzdBruhSi-Y",
    "outputId": "c3db8786-a1f7-4428-ad2f-4b3e1eeced96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aeroplane' 'bicycle' 'boat' 'bus' 'dog' 'motorbike' 'train']\n",
      "[331 418 398 272 538 390 328]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Getting all the classes and creating label encoding\n",
    "\n",
    "all_labels = []\n",
    "\n",
    "\n",
    "for out in sorted(os.listdir('VOCdev/VOC2007/Annotations/')):\n",
    "    tree = ET.parse('VOCdev/VOC2007/Annotations/' + out)\n",
    "    for obj in tree.findall('object'):\n",
    "        lab = (obj.find('name').text)\n",
    "        # all_labels.append(lab)\n",
    "        if (lab in (select_classes)):\n",
    "          all_labels.append(lab)\n",
    "        \n",
    "distict_labels = list(set(all_labels))\n",
    "distict_labels = sorted(distict_labels)\n",
    "\n",
    "#label 0 is set for background\n",
    "lab_to_val = {j:i+1 for i,j in enumerate(distict_labels)}\n",
    "val_to_lab = {i+1:j for i,j in enumerate(distict_labels)}\n",
    "\n",
    "num_classes = len(distict_labels) + 1\n",
    "\n",
    "\n",
    "print(np.unique(np.array(all_labels), return_counts=True)[0])\n",
    "print(np.unique(np.array(all_labels), return_counts=True)[1])\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-KRpBX2fOFkA"
   },
   "source": [
    "##### Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u01K3tCSXgQI"
   },
   "source": [
    "##### Create Pytorch Dataset and Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xcLsN7xSlxI"
   },
   "outputs": [],
   "source": [
    "\n",
    "class pascal_voc_data(Dataset):\n",
    "    def __init__(self, img_dir,desc_dir,type_list, isTrain, transform = None):\n",
    "        super().__init__()\n",
    "        self.img_dir = img_dir\n",
    "        self.desc_dir = desc_dir\n",
    "        self.type_list = type_list\n",
    "        self.isTrain = isTrain\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "        self.img_names = []\n",
    "        self.img_descs = []\n",
    "        for img in sorted(os.listdir(img_dir)):\n",
    "          if img[:-4] in self.type_list:\n",
    "            self.img_names.append(img)\n",
    "        \n",
    "        for desc in sorted(os.listdir(desc_dir)):\n",
    "          if desc[:-4] in  self.type_list:\n",
    "            self.img_descs.append(desc)\n",
    "       \n",
    "        self.img_names = [os.path.join(img_dir, img_name) for img_name in self.img_names]\n",
    "        self.img_descs = [os.path.join(desc_dir, img_desc) for img_desc in self.img_descs]\n",
    "\n",
    "        \n",
    "        # self.img_names = sorted(os.listdir(img_dir))\n",
    "        # self.img_descs = sorted(os.listdir(desc_dir))\n",
    "        \n",
    "        # self.img_names = [os.path.join(img_dir, img_name) for img_name in self.img_names]\n",
    "        # self.img_descs = [os.path.join(desc_dir, img_desc) for img_desc in self.img_descs]\n",
    "                \n",
    "        \n",
    "        self.loc_gts = []\n",
    "        self.loc_labels = []\n",
    "        self.final_img_names = []\n",
    "        for img_idx,img_desc in enumerate(self.img_descs):\n",
    "            tree = ET.parse(img_desc)\n",
    "            gt = []\n",
    "            loc_lab = []\n",
    "            for obj in tree.findall('object'):\n",
    "              if ((obj.find('name').text) not in (select_classes)):\n",
    "                continue\n",
    "              lab = lab_to_val[(obj.find('name').text)]\n",
    "              \n",
    "              loc1 = int(obj.find('bndbox').find('xmin').text)\n",
    "              loc2 = int(obj.find('bndbox').find('xmax').text)\n",
    "              loc3 = int(obj.find('bndbox').find('ymin').text)\n",
    "              loc4 = int(obj.find('bndbox').find('ymax').text)\n",
    "\n",
    "              # if ht or width is less than 10, ignore the gt box\n",
    "              if ((loc2 - loc1) < 10 ) or ((loc4 - loc3) < 10):\n",
    "                continue\n",
    "\n",
    "              gt.append([int(loc1),int(loc2),int(loc3),int(loc4)])\n",
    "              loc_lab.append(lab)\n",
    "            if (len(gt) == 0):\n",
    "              continue\n",
    "            self.loc_gts.append(gt)\n",
    "            self.loc_labels.append(loc_lab)\n",
    "            self.final_img_names.append(self.img_names[img_idx])\n",
    "\n",
    "        self.img_names = self.final_img_names\n",
    "             \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        img = Image.open(img_name)\n",
    "\n",
    "        arr_loc_gts = np.array(self.loc_gts[idx])\n",
    "        label = self.loc_labels[idx]\n",
    "        \n",
    "        img_h_pre = img.size[1]\n",
    "        img_w_pre = img.size[0]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        img_h_post = img.shape[1]\n",
    "        img_w_post = img.shape[2]\n",
    "        \n",
    "        height_ratio = img_h_post/img_h_pre\n",
    "        width_ratio = img_w_post/img_w_pre\n",
    "      \n",
    "        \n",
    "        \n",
    "        arr_loc_gts[:,0] = arr_loc_gts[:,0]*width_ratio\n",
    "        arr_loc_gts[:,1] = arr_loc_gts[:,1]*width_ratio\n",
    "        arr_loc_gts[:,2] = arr_loc_gts[:,2]*height_ratio\n",
    "        arr_loc_gts[:,3] = arr_loc_gts[:,3]*height_ratio\n",
    "                        \n",
    "        gts = (arr_loc_gts).tolist()\n",
    "        \n",
    "        \n",
    "        return img, gts,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7kwz4WdOKcP"
   },
   "source": [
    "##### Split into train and validations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rpt50KST3B6u"
   },
   "outputs": [],
   "source": [
    "test_images = []\n",
    "train_images = []\n",
    "\n",
    "for img in os.listdir('VOCdev/VOC2007/JPEGImages/'):\n",
    "    train_images.append(img[:-4])\n",
    "    \n",
    "for img in os.listdir('VOCtest/VOC2007/JPEGImages/'):\n",
    "    valid_images.append(img[:-4])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFYONNIQ3Hju"
   },
   "outputs": [],
   "source": [
    "train_dataset = pascal_voc_data('VOCdev/VOC2007/JPEGImages/', 'VOCdev/VOC2007/Annotations/', train_images, True)\n",
    "test_dataset = pascal_voc_data('VOCtest/VOC2007/JPEGImages/', 'VOCtest/VOC2007/Annotations/', test_images, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "UGuwJKCi4CiB",
    "outputId": "37e64c65-4f43-48de-fa70-09a1461a6d97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1777\n",
      "672\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83LUBqs7OPQ3"
   },
   "source": [
    "##### Validation class images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2IOlskQ3S8Zx"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "test_loader = DataLoader(valid_dataset, batch_size = len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ng_gQP2XX5WK"
   },
   "source": [
    "### Visualise input image with box co-ordinates provided in format - x0,x1,y0,y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBSCEoTTTAAX"
   },
   "outputs": [],
   "source": [
    "# Given input image, draw rectangles as specified by gt_box and pred_box and display\n",
    "def visualize_tensor(img, gt_box, pred_box,save_image='',tb_writer=None):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    transform_img = inv_normalize(img[0]).permute(1,2,0).to('cpu').numpy()\n",
    "    transform_img = transform_img.copy()\n",
    "    for box in gt_box:\n",
    "        x0, x1, y0, y1 = box\n",
    "        cv2.rectangle(transform_img, (int(x0),int(y0)), (int(x1),int(y1)), color=(0, 255, 255), thickness=2)\n",
    "    for box in pred_box:\n",
    "        x0, x1, y0, y1 = box\n",
    "        cv2.rectangle(transform_img, (int(x0), int(y0)), (int(x1), int(y1)), color=(255, 0, 0), thickness=2)\n",
    "    \n",
    "    if tb_writer:\n",
    "      # grid = torchvision.utils.make_grid(transform_img)\n",
    "      tb_writer.add_image(save_image, transform_img, dataformats='HWC')\n",
    "    elif save_image == '':\n",
    "        plt.imshow(transform_img)\n",
    "        plt.show()  \n",
    "    else:\n",
    "        plt.imshow(transform_img)\n",
    "        plt.savefig(save_image + '.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87N6zHd_YFh7"
   },
   "source": [
    "### Faster RCNN backbone used - VGG16 model\n",
    "\n",
    "*   Top 10 layers (top 4 conv layers) are not trained\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NUkDSEqgTLvP"
   },
   "outputs": [],
   "source": [
    "# Base VGG16 model\n",
    "\n",
    "def Faster_RCNN_vgg16(num_freeze_top): \n",
    "    vgg16 = models.vgg16(pretrained=True)\n",
    "    vgg_feature_extracter  = vgg16.features[:-1]\n",
    "    vgg_classifier = vgg16.classifier[:-1]\n",
    "    \n",
    "    # Freeze learning of top few conv layers\n",
    "    for layer in vgg_feature_extracter[:num_freeze_top]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return vgg_feature_extracter.to(device), vgg_classifier.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IkuFUER3YUGh"
   },
   "source": [
    "### Code for anchor generation on an image\n",
    "\n",
    "*   For train and test, only anchors lying inside the image boundary are considered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MeyisFcbTfvz"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    RCNN Paper\n",
    "    For anchors, we use 3 scales with box areas of (128*128) || (256*256) , and (512*512) pixels, \n",
    "    and 3 aspect ratios of 1:1, 1:2, and 2:1.\n",
    "\n",
    "'''\n",
    "\n",
    "# At point x,y from feature map, return 9 anchors on the input image scale\n",
    "def generate_anchor_at_point(x,y):\n",
    "    anchor_positions = torch.zeros((len(anchor_scale) * len(anchor_ratio),4))\n",
    "    ctr_x = (x*2+1)*(conversion_scale/2)   # for x = 0, centre is 8 || for x = 1, centre is 24\n",
    "    ctr_y = (y*2+1)*(conversion_scale/2)\n",
    "    for ratio_idx in range(len(anchor_ratio)):\n",
    "        for scale_idx in range(len(anchor_scale)):\n",
    "\n",
    "            current = len(anchor_scale)*ratio_idx + scale_idx\n",
    "            ratio = anchor_ratio[ratio_idx]\n",
    "            scale = anchor_scale[scale_idx]\n",
    "\n",
    "            h = conversion_scale*scale*torch.sqrt(ratio)\n",
    "            w = conversion_scale*scale*torch.sqrt(1.0/ratio)\n",
    "            \n",
    "            anchor_positions[current,0] = ctr_x - w/2\n",
    "            anchor_positions[current,1] = ctr_x + w/2\n",
    "            anchor_positions[current,2] = ctr_y - h/2\n",
    "            anchor_positions[current,3] = ctr_y + h/2\n",
    "            \n",
    "    return anchor_positions\n",
    "\n",
    "\n",
    "# For features of scale (x,y) , generate all the anchor boxes\n",
    "# input is height,width\n",
    "# returns output on  x*y*9,4\n",
    "def generate_anchors(x,y):\n",
    "    anchor_positions = torch.zeros((x*y,len(anchor_scale) * len(anchor_ratio),4))\n",
    "    for ctr_x in range(x):\n",
    "        for ctr_y in range(y):\n",
    "            current = ctr_x*y + ctr_y\n",
    "            anchors = generate_anchor_at_point(ctr_x, ctr_y)\n",
    "            anchor_positions[current] = anchors\n",
    "    return anchor_positions.reshape(-1,4)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJh4ImOsTq2M"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "RCNN paper\n",
    "    - During training, we ignore all cross-boundary anchors so they do not contribute to the loss\n",
    "    - During testing, however, we still apply the fully convolutional RPN to the entire image. This may \n",
    "    generate crossboundary proposal boxes, which we clip to the image boundary\n",
    "'''\n",
    "\n",
    "'''\n",
    "    For this code, cross boundary anchors are ignored at this step both in train and test\n",
    "'''\n",
    "\n",
    "def get_valid_anchors(anchor_positions):\n",
    "    valid_anchors_idx = torch.where((anchor_positions[:,0] >= 0) & \n",
    "             (anchor_positions[:,1] <= input_image_width) &\n",
    "             (anchor_positions[:,2] >= 0) &\n",
    "             (anchor_positions[:,3] <= input_image_height) )[0]\n",
    "\n",
    "    anchor_positions = anchor_positions[valid_anchors_idx]\n",
    "    return anchor_positions, valid_anchors_idx\n",
    "    \n",
    "# valid_anchors,valid_anchors_idx = get_valid_anchors(anchor_positions)\n",
    "# print(valid_anchors)\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSyI0-x9Yves"
   },
   "source": [
    "### Getting intersection over union between 2 sets of boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYmH88-_U9yj"
   },
   "outputs": [],
   "source": [
    "# Get Intersection over Union between all the boxes in anchor_positions and gt_boxes\n",
    "\n",
    "def get_iou_matrix(anchor_positions, gt_boxes):\n",
    "    iou_matrix = torch.zeros((len(anchor_positions), len(gt_boxes)))\n",
    "    for idx,box in enumerate(gt_boxes):\n",
    "        if isinstance(box,torch.Tensor):\n",
    "          gt = torch.cat([box]*len(anchor_positions)).view(1,-1,4)[0]\n",
    "        else:\n",
    "          gt = torch.FloatTensor([box]*len(anchor_positions))\n",
    "        max_x = torch.max(gt[:,0],anchor_positions[:,0])\n",
    "        min_x = torch.min(gt[:,1],anchor_positions[:,1])\n",
    "        max_y = torch.max(gt[:,2],anchor_positions[:,2])\n",
    "        min_y = torch.min(gt[:,3],anchor_positions[:,3])\n",
    "                \n",
    "        invalid_roi_idx = (min_x < max_x) | (min_y < max_y)\n",
    "        roi_area = (min_x - max_x)*(min_y - max_y)\n",
    "        roi_area[invalid_roi_idx] = 0\n",
    "        \n",
    "        total_area = (gt[:,1] - gt[:,0])*(gt[:,3] - gt[:,2]) + \\\n",
    "                    (anchor_positions[:,1] - anchor_positions[:,0])*(anchor_positions[:,3]-anchor_positions[:,2]) - \\\n",
    "                     roi_area\n",
    "                    \n",
    "        iou = roi_area/(total_area + 1e-6)\n",
    "        \n",
    "        iou_matrix[:,idx] = iou\n",
    "    return iou_matrix\n",
    "        \n",
    "        \n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kUAuHRqFZFbP"
   },
   "source": [
    "### After IOU calculation between anchors and gt boxes \n",
    "\n",
    "*   Assign +1/-1/0 labels to anchors based on IOU\n",
    "*   Sample 128 positive and 128 negative anchors for training\n",
    "* If less than 128 positive anchors, pad with negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_fvWDP7dHDr"
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "    RCNN paper: \n",
    "        assign a positive label to two kinds of anchors: \n",
    "        (i) the anchor/anchors with the highest Intersection-overUnion (IoU) overlap with a ground-truth box, \n",
    "        (ii) an anchor that has an IoU overlap higher than 0.7 with\n",
    "    \n",
    "        assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 \n",
    "        Anchors that are neither positive nor negative do not contribute to the training objective.\n",
    "        \n",
    "'''\n",
    "\n",
    "def get_max_iou_data(iou_matrix):\n",
    "    gt_max_value = iou_matrix.max(axis=0)[0] #  for each gt box, this is the max iou\n",
    "    \n",
    "    #There is a possiblilty that corresponding to one gt box, there are multiple anchors with same iou (max value)\n",
    "    all_gt_max = torch.where(iou_matrix == gt_max_value)[0]\n",
    "    \n",
    "    # For each anchor box, this is the max iou with any of the gt_box\n",
    "    anchor_max_value = torch.max(iou_matrix, axis=1)[0]\n",
    "    anchor_max = torch.argmax(iou_matrix, axis=1)\n",
    "    \n",
    "    return all_gt_max, anchor_max_value,anchor_max\n",
    "\n",
    "\n",
    "\n",
    "# 1 - positive || 0 - negative || -1 - ignore\n",
    "def get_anchor_labels(anchor_positions, all_gt_max, anchor_max_value):\n",
    "    anchor_labels = torch.zeros(anchor_positions.shape[0])\n",
    "    anchor_labels.fill_(-1.0)\n",
    "\n",
    "    # for each anchor box, if iou with any of the gt_box is less than threshold, mark as 0\n",
    "    anchor_labels[anchor_max_value < 0.3] = 0\n",
    "    \n",
    "    # If corresponding to any gt_box, the anchor has max iou -> mark as 1\n",
    "    anchor_labels[all_gt_max] = 1.0\n",
    "    \n",
    "    # If for any anchor box, iou is greater than threshold for any of the gt_box, mark as 1\n",
    "    anchor_labels[anchor_max_value > 0.7] = 1.0\n",
    "    return anchor_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LmEJvEvsdPZc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    " RCNN paper \n",
    "    randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where\n",
    "    the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer \n",
    "    than 128 positive samples in an image, we pad the mini-batch with negative ones.\n",
    "'''\n",
    "\n",
    "def sample_anchors_for_train(anchor_labels):\n",
    "    pos_anchor_labels = torch.where(anchor_labels == 1)[0]\n",
    "    num_pos = min(num_anchors_sample/2, len(pos_anchor_labels))\n",
    "    pos_idx = np.random.choice(pos_anchor_labels,  int(num_pos), replace=False)\n",
    "\n",
    "    neg_anchor_labels = torch.where(anchor_labels == 0)[0]\n",
    "    num_neg = num_anchors_sample - num_pos\n",
    "    neg_idx = np.random.choice(neg_anchor_labels, int(num_neg), replace=False)\n",
    "    \n",
    "    anchor_labels[:] = -1\n",
    "    anchor_labels[pos_idx] = 1\n",
    "    anchor_labels[neg_idx] = 0\n",
    "    \n",
    "    return anchor_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9WlodDVZiAp"
   },
   "source": [
    "### bbox regression training data calculation\n",
    "* delta_anchor_gt - Get delta between anchor_positions and gt_boxes\n",
    "* correct_anchor_positions - Given delta and anchor_positions, correct the anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5uwFrcHmdQB6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    x,y,w,h are ctr_x, ctr_y, width and height for GT box\n",
    "    dx = (x - x_{a})/w_{a}\n",
    "    dy = (y - y_{a})/h_{a}\n",
    "    dw = log(w/ w_a)\n",
    "    dh = log(h/ h_a)\n",
    "'''\n",
    "\n",
    "# Get delta between anchor_positions and gt_boxes\n",
    "def delta_anchor_gt(anchor_positions, gt_boxes , anchor_max):\n",
    "\n",
    "    anchor_gt_map =  gt_boxes[anchor_max]\n",
    "    \n",
    "    anchor_height = anchor_positions[:,3] - anchor_positions[:,2] # y2-y1\n",
    "    anchor_width =  anchor_positions[:,1] - anchor_positions[:,0] # x2-x1\n",
    "    anchor_ctr_y = anchor_positions[:,2] + anchor_height/2 # y1 + h/2\n",
    "    anchor_ctr_x = anchor_positions[:,0] + anchor_width/2  # x1 + w/2\n",
    "    \n",
    "    gt_height = anchor_gt_map[:,3] - anchor_gt_map[:,2] # y2-y1\n",
    "    gt_width =  anchor_gt_map[:,1] - anchor_gt_map[:,0] # x2-x1\n",
    "    gt_ctr_y = anchor_gt_map[:,2] + gt_height/2 # y1 + h/2\n",
    "    gt_ctr_x = anchor_gt_map[:,0] + gt_width/2  # x1 + w/2\n",
    "    \n",
    "    dx = (gt_ctr_x - anchor_ctr_x)/anchor_width\n",
    "    dy = (gt_ctr_y - anchor_ctr_y)/anchor_height\n",
    "    dw = torch.log(gt_width/anchor_width)\n",
    "    dh = torch.log(gt_height/anchor_height)\n",
    "    \n",
    "    delta = torch.zeros_like(anchor_positions)\n",
    "    delta[:,0] = dx\n",
    "    delta[:,1] = dy\n",
    "    delta[:,2] = dw\n",
    "    delta[:,3] = dh\n",
    "   \n",
    "    return delta\n",
    "\n",
    "\n",
    "# Given delta and anchor_positions, correct the anchors\n",
    "def correct_anchor_positions(anchor_positions, delta):\n",
    "    ha = anchor_positions[:,3] - anchor_positions[:,2] # y2-y1\n",
    "    wa =  anchor_positions[:,1] - anchor_positions[:,0] # x2-x1\n",
    "    ya = anchor_positions[:,2] + ha/2 # y1 + h/2\n",
    "    xa = anchor_positions[:,0] + wa/2  # x1 + w/2\n",
    "    \n",
    "    dx = delta[:,0]\n",
    "    dy = delta[:,1]\n",
    "    dw = delta[:,2]\n",
    "    dh = delta[:,3]\n",
    "    \n",
    "    \n",
    "    x = dx*wa + xa\n",
    "    y = dy*ha + ya\n",
    "    w = torch.exp(dw)*wa\n",
    "    h = torch.exp(dh)*ha\n",
    "    \n",
    "    correct_anchor_positions = torch.zeros_like(anchor_positions)\n",
    "    \n",
    "    correct_anchor_positions[:,0] = x - w/2\n",
    "    correct_anchor_positions[:,1] = x + w/2\n",
    "    correct_anchor_positions[:,2] = y - h/2\n",
    "    correct_anchor_positions[:,3] = y + h/2\n",
    "    \n",
    "    return correct_anchor_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77rqSE9zZ5yq"
   },
   "source": [
    "### Faster RCNN region proposal network defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IbYuoWk7x1xk"
   },
   "outputs": [],
   "source": [
    "# Region Proposal Network\n",
    "\n",
    "class Faster_RCNN_rpn(nn.Module):\n",
    "    def __init__(self,extracter):\n",
    "        super().__init__()\n",
    "        self.extracter = extracter\n",
    "        self.conv1 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "        #class_conv1 checks corresponding to 1 point in feature map, 18 outputs. \n",
    "        # 9 anchors * (2) || anchor has object or not\n",
    "        self.class_conv = nn.Conv2d(512, 2*len(anchor_scale)*len(anchor_ratio), 1, 1, 0)  \n",
    "        #reg_conv1 checks corresponding to 1 point in feature map, 36 outputs. \n",
    "        # 9 anchors * (4) || anchor delta wrt ground truth boxes\n",
    "        self.reg_conv = nn.Conv2d(512, 4*len(anchor_scale)*len(anchor_ratio), 1 ,1 , 0)\n",
    "\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        self.class_conv.weight.data.normal_(0, 0.01)\n",
    "        self.class_conv.bias.data.zero_()\n",
    "        self.reg_conv.weight.data.normal_(0, 0.01)\n",
    "        self.reg_conv.bias.data.zero_()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        # input and output features of CNN are (nSamples x nChannels x Height x Width)\n",
    "        \n",
    "        features = self.extracter(x)\n",
    "        conv1_out = F.relu(self.conv1(features))\n",
    "        class_out = self.class_conv(conv1_out)\n",
    "        reg_out = self.reg_conv(conv1_out)\n",
    "        \n",
    "        return features, class_out, reg_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KuaRnupHaC2Q"
   },
   "source": [
    "### Loss calculation - The same loss function is used for decider and RPN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XRC_TDXZx_hb"
   },
   "outputs": [],
   "source": [
    "# Loss for Region Proposal Netwoek\n",
    "# Same loss is used for decider network\n",
    "\n",
    "def RPN_loss(locs_preditct, class_predict, final_RPN_locs,final_RPN_class):\n",
    "    final_RPN_locs = final_RPN_locs.to(device)\n",
    "    final_RPN_class = final_RPN_class.long().to(device)\n",
    "    \n",
    "    # Cross entropy loss (check if the target is background or foreground)\n",
    "    # Only consider labels with values 1 or 0. ignore -1\n",
    "    class_loss = F.cross_entropy(class_predict, final_RPN_class, ignore_index=-1)\n",
    "\n",
    "    #smooth L1 regression loss (calculate the loss in predicted locations of foreground)\n",
    "    '''\n",
    "        Smooth L1 loss uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise\n",
    "    '''\n",
    "\n",
    "    foreground_class_idx = (final_RPN_class > 0)\n",
    "    locs_preditct  = locs_preditct[foreground_class_idx]\n",
    "    final_RPN_locs = final_RPN_locs[foreground_class_idx]\n",
    "\n",
    " \n",
    "    \n",
    "    loc_loss = F.smooth_l1_loss(locs_preditct, final_RPN_locs) / (sum(foreground_class_idx)+1e-6)\n",
    "\n",
    "    \n",
    "    rpn_loss = class_loss + rpn_loss_lambda*loc_loss\n",
    "  \n",
    "    return rpn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2LAvoeviaKLd"
   },
   "source": [
    "### Non Maximum Suppression\n",
    "* Custom implemented method is implemented but not used\n",
    "* Pytorch nms method gives ~5-6 times speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZPXzkHbdUai"
   },
   "outputs": [],
   "source": [
    "# Apply non max suppression on anchors\n",
    "\n",
    "def non_max_suppression(correct_anchor_positions, class_score, img_h, img_w, isTrain):\n",
    "    \n",
    "    if isTrain:\n",
    "        nms_pre = nms_num_train_pre\n",
    "        nms_post = nms_num_train_post\n",
    "    else :\n",
    "        nms_pre = nms_num_test_pre\n",
    "        nms_post = nms_num_test_post\n",
    "    \n",
    "    \n",
    "    # Clip the anchors to image dimensions\n",
    "    correct_anchor_positions[correct_anchor_positions[:,0] < 0,0] = 0 # x1\n",
    "    correct_anchor_positions[correct_anchor_positions[:,1] > img_w,1] = img_w # x2\n",
    "    correct_anchor_positions[correct_anchor_positions[:,2] < 0,2] = 0 # y1\n",
    "    correct_anchor_positions[correct_anchor_positions[:,3] > img_h,3] = img_h # y2\n",
    "    \n",
    "    \n",
    "    # Only keep anchors with height and width > nms_min_size\n",
    "    anchor_width = correct_anchor_positions[:,1] - correct_anchor_positions[:,0]\n",
    "    anchor_height = correct_anchor_positions[:,3] - correct_anchor_positions[:,2]\n",
    "\n",
    "\n",
    "    \n",
    "    keep_idx = (anchor_height > nms_min_size) & (anchor_width > nms_min_size)\n",
    "    correct_anchor_positions = correct_anchor_positions[keep_idx]\n",
    "    class_score = class_score[keep_idx]\n",
    "\n",
    "    # Get the index of sorted class scores in descending order and select top nms_pre idx\n",
    "    sorted_class_scores = torch.argsort(class_score, descending=True)\n",
    "    pre_nms_idx = sorted_class_scores[:nms_pre]\n",
    "    correct_anchor_positions = correct_anchor_positions[pre_nms_idx]\n",
    "    class_score = class_score[pre_nms_idx]\n",
    "    \n",
    "    # Implementation for non max suppression\n",
    "    '''\n",
    "    sorted_class_scores = torch.argsort(class_score, descending=True).to('cpu')\n",
    "    keep_anchors = []\n",
    "    \n",
    "\n",
    "    Apply NMS\n",
    "    while len(sorted_class_scores) > 1:\n",
    "        current = sorted_class_scores[0]\n",
    "        keep_anchors.append(current)\n",
    "        iou_matrix = get_iou_matrix(correct_anchor_positions[sorted_class_scores[1:]],correct_anchor_positions[current].reshape(1,-1,4)[0])\n",
    "        sorted_class_scores = sorted_class_scores[np.where(iou_matrix < nms_threshold)[0] + 1]\n",
    "    \n",
    "    if (len(sorted_class_scores) == 1):\n",
    "        keep_anchors.append(sorted_class_scores[0])\n",
    "    '''\n",
    "\n",
    "\n",
    "    '''\n",
    "      using pytorch standard nms function as it gives 5-6 times speedup\n",
    "    '''\n",
    "    change_format = torch.zeros_like(correct_anchor_positions)\n",
    "    change_format[:,0] = correct_anchor_positions[:,0]\n",
    "    change_format[:,1] = correct_anchor_positions[:,2]\n",
    "    change_format[:,2] = correct_anchor_positions[:,1]\n",
    "    change_format[:,3] = correct_anchor_positions[:,3]\n",
    "\n",
    "    keep_anchors = nms(change_format.to('cpu'), class_score.clone().detach().to('cpu'), nms_threshold)\n",
    "\n",
    "    keep_anchors = keep_anchors[:nms_post]\n",
    "    correct_anchor_positions = correct_anchor_positions[keep_anchors]\n",
    "    \n",
    "    return correct_anchor_positions\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qHwtmTUzacBH"
   },
   "source": [
    "### Assign classes to output of Region Proposal Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVDS5ryzh6PG"
   },
   "outputs": [],
   "source": [
    "def assign_classification_anchors(extracted_roi, gt_boxes, gt_labels, isTrain):\n",
    "\n",
    "    \n",
    "    # calculate iou of rois and gt boxes\n",
    "    iou_matrix = get_iou_matrix(extracted_roi, gt_boxes)\n",
    "    \n",
    "    # for each ROI, find gt with max iou and corresponding value\n",
    "    gt_roi_argmax = iou_matrix.argmax(axis=1)\n",
    "    gt_roi_max = iou_matrix.max(axis=1)[0]\n",
    "    \n",
    "    #If a particular ROI has max overlap with a gt_box, assign label of gt_box to roi\n",
    "    assign_labels = gt_labels[gt_roi_argmax]\n",
    "    \n",
    "    num_pos = pt_n_sample*pt_pos_ratio\n",
    "    pos_idx = torch.where(gt_roi_max > pt_pos_iou_threshold)[0]\n",
    "    if isTrain:\n",
    "      pos_idx = np.random.choice(pos_idx, int(min(len(pos_idx), num_pos)), replace=False)\n",
    "    \n",
    "    \n",
    "    num_neg = pt_n_sample - len(pos_idx)\n",
    "    neg_idx = torch.where(gt_roi_max < pt_neg_iou_threshold)[0]\n",
    "    if isTrain:\n",
    "      neg_idx = np.random.choice(neg_idx, int(min(len(neg_idx), num_neg)), replace=False)\n",
    "    \n",
    "    \n",
    "    keep_idx = np.append(pos_idx, neg_idx)\n",
    "    assign_labels[neg_idx] = 0\n",
    "    assign_labels = assign_labels[keep_idx]\n",
    "    extracted_roi = extracted_roi[keep_idx]\n",
    "    gt_roi_argmax = gt_roi_argmax[keep_idx]\n",
    "\n",
    "    \n",
    "    return assign_labels, extracted_roi,gt_roi_argmax, keep_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-1clxYZamee"
   },
   "source": [
    "### ROI Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zBYJHMOaZi9"
   },
   "outputs": [],
   "source": [
    "def ROI_pooling(extracted_roi, feature,ROI_pooling_layer):\n",
    "    extracted_roi = extracted_roi/16.0\n",
    "    out = []\n",
    "    for roi in extracted_roi:\n",
    "        \n",
    "        x1 = int(roi[0])\n",
    "        x2 = int(roi[1]+1)\n",
    "        y1 = int(roi[2])\n",
    "        y2 = int(roi[3]+1)\n",
    "        out.append(ROI_pooling_layer(feature[:,:,y1:y2,x1:x2]))\n",
    "    out = torch.cat(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwYUmNDOaqPB"
   },
   "source": [
    "### Faster RCNN decider layer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmDFngjsiJ43"
   },
   "outputs": [],
   "source": [
    "class Faster_RCNN_decider(nn.Module):\n",
    "    def __init__(self,classifier):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = classifier\n",
    "        self.class_lin = nn.Linear(4096, num_classes)\n",
    "        self.reg_lin = nn.Linear(4096, num_classes*4)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        decider_class = self.class_lin(x)\n",
    "        decider_loc = self.reg_lin(x)\n",
    "        \n",
    "        return decider_class, decider_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynpLMBOSauXS"
   },
   "source": [
    "### Pipeline (train/validation) for a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Hs58JpWiNlc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "img_anchors_all = generate_anchors(int(input_image_height/conversion_scale), int(input_image_width/conversion_scale)).to(device)\n",
    "\n",
    "def single_image_pipeline(input_image, gt_box, label,isTrain):\n",
    "\n",
    "    input_image = input_image.to(device)\n",
    "    gt_box = torch.FloatTensor(gt_box).to(device)\n",
    "    label = torch.FloatTensor(label).to(device)\n",
    "\n",
    "    # Generate CNN features for input image\n",
    "    # Genearate region proposals predictions\n",
    "    features, class_out, reg_out = rpn(input_image)\n",
    "\n",
    "\n",
    "    locs_preditct = reg_out.permute(0,2,3,1).reshape(1,-1,4)[0]\n",
    "    class_predict = class_out.permute(0,2,3,1).reshape(1,-1,2)[0]\n",
    "    class_score = class_out.reshape(1,features.shape[2],features.shape[3],9,2)[:,:,:,:,1].reshape(1,-1)[0].detach()\n",
    "\n",
    "    \n",
    "   \n",
    "    # For training region proposal network, generate anchors on the image. \n",
    "    img_anchors_valid, img_anchors_valid_idx = get_valid_anchors(img_anchors_all.clone())\n",
    "\n",
    "    iou_anchors_gt = get_iou_matrix(img_anchors_valid, gt_box)\n",
    "    all_gt_max, anchor_max_value,anchor_max = get_max_iou_data(iou_anchors_gt)\n",
    "    \n",
    "\n",
    "    anchor_labels = get_anchor_labels(img_anchors_valid, all_gt_max, anchor_max_value)\n",
    "    anchor_labels = sample_anchors_for_train(anchor_labels)\n",
    "    \n",
    "    # TODO - check if correct. - Done\n",
    "    delta = delta_anchor_gt(img_anchors_valid, gt_box, anchor_max)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    final_RPN_locs = torch.zeros_like(img_anchors_all)\n",
    "    final_RPN_locs[img_anchors_valid_idx] = delta\n",
    "\n",
    "    final_RPN_class = torch.zeros(img_anchors_all.shape[0])\n",
    "    final_RPN_class.fill_(-1)\n",
    "    final_RPN_class[img_anchors_valid_idx] = anchor_labels\n",
    "\n",
    "    # Loss for RPN layer\n",
    "    loss1 = RPN_loss(locs_preditct, class_predict, final_RPN_locs, final_RPN_class).to(device)\n",
    "    \n",
    "    # Based on the bbox output of rpn, correct the generated anchors\n",
    "    corrected_anchors = correct_anchor_positions(img_anchors_all.to(device), locs_preditct).detach()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Apply nms on the region proposals\n",
    "    extracted_rois = non_max_suppression(corrected_anchors, class_score, input_image.shape[2],input_image.shape[3],isTrain)\n",
    "\n",
    "   \n",
    "    final_decider_class, extracted_roi_samples,gt_roi_argmax,idx = assign_classification_anchors(extracted_rois, gt_box, label, isTrain)\n",
    "\n",
    "    final_decider_locs = delta_anchor_gt(extracted_roi_samples, gt_box,gt_roi_argmax)\n",
    "   \n",
    "    # Apply ROI pooling on the extracted ROIs\n",
    "    pooled_features = ROI_pooling(extracted_roi_samples, features, ROI_pooling_layer)\n",
    "\n",
    "    decider_class, decider_loc = decider(pooled_features)\n",
    "    decider_loc = decider_loc.reshape(pooled_features.shape[0],-1,4) # 128*21*4\n",
    "    decider_loc = decider_loc[torch.arange(0,pooled_features.shape[0]), final_decider_class.long()] # 128*4\n",
    "    \n",
    "    # Loss for decider layer\n",
    "    loss2 = RPN_loss(decider_loc, decider_class, final_decider_locs, final_decider_class).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        decider_loc_no_grad = decider_loc.clone().to(device)\n",
    "\n",
    "    # Correct the ROIs based on bbox output for decider layers\n",
    "    corrected_roi = correct_anchor_positions(extracted_roi_samples,decider_loc_no_grad).detach()\n",
    "    \n",
    "    return loss1, loss2 , decider_class, corrected_roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hei6_f-5Ougk"
   },
   "source": [
    "#### Test model for input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JtmTEll_YWD1"
   },
   "outputs": [],
   "source": [
    "def test_model(input_image):\n",
    "  rpn.eval()\n",
    "  decider.eval()\n",
    "\n",
    "  input_image = input_image.to(device)\n",
    "  features, class_out, reg_out = rpn(input_image)\n",
    "  locs_preditct = reg_out.permute(0,2,3,1).reshape(1,-1,4)[0]\n",
    "  class_score = class_out.reshape(1,features.shape[2],features.shape[3],9,2)[:,:,:,:,1].reshape(1,-1)[0].detach()\n",
    "\n",
    "  corrected_anchors = correct_anchor_positions(img_anchors_all.to(device), locs_preditct).detach()\n",
    "\n",
    "    \n",
    "  extracted_rois = non_max_suppression(corrected_anchors, class_score, input_image.shape[2],input_image.shape[3],False)\n",
    "\n",
    "\n",
    "  pooled_features = ROI_pooling(extracted_rois, features, ROI_pooling_layer)\n",
    "\n",
    "  decider_class, decider_loc = decider(pooled_features)\n",
    "  \n",
    "  with torch.no_grad():\n",
    "        decider_loc_no_grad = decider_loc.clone().to(device)\n",
    "\n",
    "  corrected_roi = correct_anchor_positions(extracted_rois,decider_loc_no_grad).detach()\n",
    "  corrected_roi[corrected_roi[:,0] < 0,0] = 0 # x1\n",
    "  corrected_roi[corrected_roi[:,1] > input_image_width,1] = input_image_width # x2\n",
    "  corrected_roi[corrected_roi[:,2] < 0,2] = 0 # y1\n",
    "  corrected_roi[corrected_roi[:,3] > input_image_height,3] = input_image_height # y2\n",
    "\n",
    "  print(corrected_roi)\n",
    "\n",
    "  decoder_conf = decider_class.softmax(dim=1).max(dim=1)[0]\n",
    "  decoder_conf = decoder_conf.detach()\n",
    "  decoder_conf = decoder_conf[decider_class.argmax(axis=1) != 0]\n",
    "  \n",
    "\n",
    "  keep_anchors = []\n",
    "  sorted_class_scores = torch.argsort(decoder_conf, descending=True)\n",
    "\n",
    "  while len(sorted_class_scores) > 1:\n",
    "    current = sorted_class_scores[0]\n",
    "    keep_anchors.append(current.item())\n",
    "    iou_matrix = get_iou_matrix(corrected_roi[sorted_class_scores[1:]],corrected_roi[current].reshape(1,-1,4)[0])\n",
    "    sorted_class_scores = sorted_class_scores[np.where(iou_matrix < 0.2)[0] + 1]\n",
    "\n",
    "  if (len(sorted_class_scores) == 1):\n",
    "    keep_anchors.append(sorted_class_scores[0].item())\n",
    "\n",
    "  for pred in decider_class.argmax(axis=1)[decider_class.argmax(axis=1) != 0][keep_anchors]:\n",
    "        print(val_to_lab[int(pred)], end=', ')\n",
    "  print('')\n",
    "  \n",
    "  for pred in decoder_conf[keep_anchors]:\n",
    "        print(pred.item(), end=', ')\n",
    "\n",
    "  visualize_tensor(input_image,extracted_rois[decider_class.argmax(axis=1) != 0][keep_anchors], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zI1Y_uq5ayp9"
   },
   "source": [
    "### Use a single image pipeline multiple times for training or batch testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFCdQQ-ybL9Y"
   },
   "source": [
    "##### Define variables, models, load saved models or training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lcWUbnTVbfNS"
   },
   "outputs": [],
   "source": [
    "load_model = ''\n",
    "\n",
    "\n",
    "\n",
    "loss1_hist = []\n",
    "loss2_hist = []\n",
    "loss_hist = []\n",
    "valid_loss1_hist = []\n",
    "valid_loss2_hist = []\n",
    "valid_loss_hist = []\n",
    "epoch_start = 0\n",
    "best_valid_score = 10000\n",
    "\n",
    "\n",
    "vgg_feature_extracter, vgg_classifier = Faster_RCNN_vgg16(num_freeze_top=10)\n",
    "rpn = Faster_RCNN_rpn(vgg_feature_extracter).to(device)\n",
    "ROI_pooling_layer = nn.AdaptiveMaxPool2d(roi_size).to(device)\n",
    "decider = Faster_RCNN_decider(vgg_classifier).to(device)\n",
    "all_params = list(list(rpn.parameters()) + list(decider.parameters()))\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(all_params, lr=0.00005)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if load_model != '':\n",
    "  print('loading model ... ')\n",
    "  checkpoint = torch.load(load_model, map_location=device)\n",
    "  rpn.load_state_dict(checkpoint['rpn_state_dict'])\n",
    "  decider.load_state_dict(checkpoint['decider_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  loss1_hist = checkpoint['loss1_hist']\n",
    "  loss2_hist = checkpoint['loss2_hist']\n",
    "  loss_hist = checkpoint['loss_hist']\n",
    "  valid_loss1_hist =checkpoint['valid_loss1_hist']\n",
    "  valid_loss2_hist = checkpoint['valid_loss2_hist']\n",
    "  valid_loss_hist = checkpoint['valid_loss_hist']\n",
    "  epoch_start = checkpoint['epoch_start']\n",
    "  best_valid_score = checkpoint['best_valid_score']\n",
    "  rpn.train()\n",
    "  decider.train()\n",
    "\n",
    "  print('model loaded ...' )\n",
    "\n",
    "\n",
    "running_count = 0\n",
    "running_net_loss = 0\n",
    "running_loss1 = 0\n",
    "running_loss2 = 0\n",
    "loss_avg_step = 20\n",
    "train_visualise_step  =150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OyQtSvA6bs3t"
   },
   "source": [
    "##### Faster RCNN train/validation code (Supported batch size is 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOfnyBNnvWcT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch - 0\n",
      "best_valid_score - 10000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_start,epoch_start\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m      4\u001b[0m     train2_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_loader2\u001b[49m, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      7\u001b[0m         img, gt_box, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m      9\u001b[0m         loss1, loss2, pred_class, pred_box \u001b[38;5;241m=\u001b[39m single_image_pipeline(img\u001b[38;5;241m.\u001b[39mto(device), gt_box, labels, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader2' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Start epoch - \" + str(epoch_start))\n",
    "print(\"best_valid_score - \" + str(best_valid_score))\n",
    "for epoch in range(epoch_start,epoch_start+50):\n",
    "    train2_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    for i, data in enumerate(train_loader2, 0):\n",
    "\n",
    "        img, gt_box, labels = data\n",
    "\n",
    "        loss1, loss2, pred_class, pred_box = single_image_pipeline(img.to(device), gt_box, labels, True)\n",
    "        net_loss = loss1 + loss2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        net_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss1 = loss1.detach()\n",
    "        loss2 = loss2.detach()\n",
    "        pred_class = pred_class.detach()\n",
    "        pred_box = pred_box.detach()\n",
    "        net_loss = net_loss.detach()\n",
    "        if not((math.isnan(net_loss)) or (math.isnan(loss1)) or (math.isnan(loss2))):\n",
    "            running_count += 1\n",
    "            running_net_loss += net_loss.data\n",
    "            running_loss1 += loss1.data\n",
    "            running_loss2 += loss2.data\n",
    "\n",
    "        \n",
    "        if ((i+1)%(loss_avg_step) == 0):\n",
    "            loss1_hist.append(running_loss1/(running_count + 1e-6))\n",
    "            loss2_hist.append(running_loss2/(running_count + 1e-6))\n",
    "            loss_hist.append(running_net_loss/(running_count + 1e-6))\n",
    "            print(str(epoch) + '--' + str(i) + '--- ' + str(running_net_loss/(running_count + 1e-6)))\n",
    "            running_count = 0\n",
    "            running_net_loss = 0\n",
    "            running_loss1 = 0\n",
    "            running_loss2 = 0\n",
    "\n",
    "\n",
    "        if ((i+1)%(train_visualise_step)) == 0:\n",
    "            print(\"Train Data Visualise\")\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 5))\n",
    "            ax1.plot(loss1_hist)\n",
    "            ax1.title.set_text('Train RPN Loss')\n",
    "            ax2.plot(loss2_hist)\n",
    "            ax2.title.set_text('Train Decider Loss')\n",
    "            ax3.plot(loss_hist)\n",
    "            ax3.title.set_text('Train Net Loss')\n",
    "            visualize_tensor(img, pred_box[pred_class.argmax(axis=1) != 0], gt_box)\n",
    "            for pred in pred_class.argmax(axis=1)[pred_class.argmax(axis=1) != 0]:\n",
    "                print(val_to_lab[int(pred)], end=', ')\n",
    "            print('')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    '''\n",
    "    Validation Code start\n",
    "    '''\n",
    "\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=True)\n",
    "    rpn.eval()\n",
    "    decider.eval()\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"Evaluating valid sets\")\n",
    "    \n",
    "    # test_model(iter(valid_loader).next()[0])\n",
    "    \n",
    "    valid_loss1 = 0\n",
    "    valid_loss2 = 0\n",
    "    valid_net_loss = 0\n",
    "    valid_run_count = 0\n",
    "    for valid_idx, valid_data in (enumerate(valid_loader,0)):\n",
    "        img, gt_box, labels = valid_data\n",
    "        loss1, loss2, pred_class, pred_box = single_image_pipeline(img.to(device), gt_box, labels, False)\n",
    "        net_loss = loss1 + loss2\n",
    "        \n",
    "        if not((math.isnan(net_loss)) or (math.isnan(loss1)) or (math.isnan(loss2))):\n",
    "            valid_loss1 += loss1.data\n",
    "            valid_loss2 += loss2.data\n",
    "            valid_net_loss += net_loss.data\n",
    "            valid_run_count += 1\n",
    "\n",
    "    \n",
    "    valid_loss1_hist.append(valid_loss1/(valid_run_count + 1e-6))\n",
    "    valid_loss2_hist.append(valid_loss2/(valid_run_count+ 1e-6))\n",
    "    valid_loss_hist.append(valid_net_loss/(valid_run_count+ 1e-6))\n",
    "    \n",
    "\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"Validation Data Visualise -- \")\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"-------------------------------\")\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 5))\n",
    "    ax1.plot(valid_loss1_hist)\n",
    "    ax1.title.set_text('Valid RPN Loss')\n",
    "    ax2.plot(valid_loss2_hist)\n",
    "    ax2.title.set_text('Valid Decider Loss')\n",
    "    ax3.plot(valid_loss_hist)\n",
    "    ax3.title.set_text('Valid Net Loss')\n",
    "    test_model(img)\n",
    "    for pred in pred_class.argmax(axis=1)[pred_class.argmax(axis=1) != 0]:\n",
    "        print(val_to_lab[int(pred)], end=', ')\n",
    "    print('')\n",
    "    plt.show()\n",
    "        \n",
    "    \n",
    "        \n",
    "    print(valid_run_count)\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"-------------------------------\")\n",
    "    rpn.train()\n",
    "    decider.train()\n",
    "\n",
    "    '''\n",
    "    Validation Code end\n",
    "    '''\n",
    "\n",
    "    PATH = 'drive/My Drive/saved_models_faster_rcnn/current.pt'\n",
    "\n",
    "    print(str(epoch)+ '--' + str(i) + ' saving model ' + PATH)\n",
    "    torch.save({\n",
    "        'rpn_state_dict': rpn.state_dict(),\n",
    "        'decider_state_dict': decider.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss1_hist':loss1_hist,\n",
    "        'loss2_hist':loss2_hist,\n",
    "        'loss_hist':loss_hist,\n",
    "        'valid_loss1_hist': valid_loss1_hist ,\n",
    "        'valid_loss2_hist':valid_loss2_hist,\n",
    "        'valid_loss_hist': valid_loss_hist,\n",
    "        'epoch_start':epoch,\n",
    "        'best_valid_score': best_valid_score\n",
    "    }, PATH)\n",
    "\n",
    "    \n",
    "    valid_net_loss = valid_net_loss/(valid_run_count+ 1e-6)\n",
    "\n",
    "    if (valid_net_loss < best_valid_score):\n",
    "        best_valid_score = valid_net_loss\n",
    "        print(\"-------------------------------\")\n",
    "        print(\"-------------------------------\")\n",
    "        print(\"Found new Best :)\")\n",
    "        print(\"-------------------------------\")\n",
    "        print(\"-------------------------------\")\n",
    "        PATH = 'drive/My Drive/saved_models_faster_rcnn/best.pt'\n",
    "\n",
    "        print(str(epoch)+ '--' + str(i) + ' saving model ' + PATH)\n",
    "        torch.save({\n",
    "            'rpn_state_dict': rpn.state_dict(),\n",
    "            'decider_state_dict': decider.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss1_hist':loss1_hist,\n",
    "            'loss2_hist':loss2_hist,\n",
    "            'loss_hist':loss_hist,\n",
    "            'valid_loss1_hist': valid_loss1_hist ,\n",
    "            'valid_loss2_hist':valid_loss2_hist,\n",
    "            'valid_loss_hist': valid_loss_hist,\n",
    "            'epoch_start':epoch,\n",
    "            'best_valid_score': best_valid_score\n",
    "        }, PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'JpegImageFile' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_batch, gt_bboxes_batch, gt_classes_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      2\u001b[0m     img_data_all \u001b[38;5;241m=\u001b[39m img_batch\n\u001b[1;32m      3\u001b[0m     gt_bboxes_all \u001b[38;5;241m=\u001b[39m gt_bboxes_batch\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dml/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dml/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dml/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dml/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[81], line 79\u001b[0m, in \u001b[0;36mpascal_voc_data.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     77\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[0;32m---> 79\u001b[0m img_h_post \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     80\u001b[0m img_w_post \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     82\u001b[0m height_ratio \u001b[38;5;241m=\u001b[39m img_h_post\u001b[38;5;241m/\u001b[39mimg_h_pre\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'JpegImageFile' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for img_batch, gt_bboxes_batch, gt_classes_batch in train_loader:\n",
    "    img_data_all = img_batch\n",
    "    gt_bboxes_all = gt_bboxes_batch\n",
    "    gt_classes_all = gt_classes_batch\n",
    "    break\n",
    "    \n",
    "img_data_all = img_data_all[:2]\n",
    "gt_bboxes_all = gt_bboxes_all[:2]\n",
    "gt_classes_all = gt_classes_all[:2]\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "req_layers = list(model.children())[:8]\n",
    "backbone = nn.Sequential(*req_layers)\n",
    "# unfreeze all the parameters\n",
    "for param in backbone.named_parameters():\n",
    "    param[1].requires_grad = True\n",
    "# run the image through the backbone\n",
    "out = backbone(img_data_all)\n",
    "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
    "out_c, out_h, out_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bMwrj8jOQUM9"
   },
   "source": [
    "#### Test model on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q2EY844far01"
   },
   "outputs": [],
   "source": [
    "img = Image.open('drive/My Drive/saved_models/bike.jpg')\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "test_model(img)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Faster-RCNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
