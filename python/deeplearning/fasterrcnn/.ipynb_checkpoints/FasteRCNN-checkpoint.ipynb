{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "dad4ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision.ops import nms\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "fd9c47e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "2ced5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/Users/shivneshwarvelayutham/Downloads/fasterrcnn/VOCtrain/VOC2007/\"\n",
    "\n",
    "vehicle_classes = {'aeroplane', 'bicycle', 'boat', 'bus', 'train', 'motorbike'}\n",
    "\n",
    "image_dim = 800\n",
    "anchor_scale = torch.FloatTensor([8,16,32])  \n",
    "anchor_ratio = torch.FloatTensor([0.5,1,2])\n",
    "conversion_scale = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "1cb43d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for file in os.listdir(data_folder + \"Annotations/\"):\n",
    "    xml_tree = ET.parse(data_folder + \"Annotations/\" + file)\n",
    "    for obj in xml_tree.findall('object') :\n",
    "        label = obj.find('name').text\n",
    "        if label in vehicle_classes:\n",
    "            labels.append(label)\n",
    "            \n",
    "distinct_labels = sorted(set(labels))\n",
    "\n",
    "label_to_id = {j:i+1 for i,j in enumerate(distinct_labels)}\n",
    "id_to_label = {i+1:j for i,j in enumerate(distinct_labels)}\n",
    "\n",
    "num_classes = len(distinct_labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2a5a4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pascal_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, transform):\n",
    "        super().__init__()\n",
    "        self.dir_path = path\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "        for i in sorted(os.listdir(path + \"JPEGImages/\")):\n",
    "            self.images.append(os.path.join(path + \"JPEGImages/\", i))\n",
    "        for i in sorted(os.listdir(path + \"Annotations/\")):\n",
    "            self.annotations.append(os.path.join(path + \"Annotations/\", i))\n",
    "            \n",
    "        self.labels = []\n",
    "        self.boxes = []\n",
    "        files = []\n",
    "        for i, ann in enumerate(self.annotations):\n",
    "            xml_tree = ET.parse(ann)\n",
    "            ll = []\n",
    "            cs = []\n",
    "            for obj in xml_tree.findall('object'):\n",
    "                label = obj.find('name').text\n",
    "                if label not in vehicle_classes:\n",
    "                    continue\n",
    "                label_id = label_to_id[label]\n",
    "                xmin = int(obj.find('bndbox').find('xmin').text)\n",
    "                xmax = int(obj.find('bndbox').find('xmax').text)\n",
    "                ymin = int(obj.find('bndbox').find('ymin').text)\n",
    "                ymax = int(obj.find('bndbox').find('ymax').text)\n",
    "                \n",
    "                if (xmax - xmin) < 10 or (ymax - ymin) < 10:\n",
    "                    continue\n",
    "                cs.append([xmin, xmax, ymin, ymax])\n",
    "                ll.append(label_id)\n",
    "                \n",
    "            if(len(ll) == 0):\n",
    "                continue\n",
    "                \n",
    "            self.labels.append(ll)\n",
    "            self.boxes.append(cs)\n",
    "            files.append(self.images[i])\n",
    "            \n",
    "        self.images = files\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.images[idx]\n",
    "        image = Image.open(image_name)\n",
    "            \n",
    "        box = np.array(self.boxes[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        img_h_pre = image.size[1]\n",
    "        img_w_pre = image.size[0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        img_h_post = image.shape[1]\n",
    "        img_w_post = image.shape[2]\n",
    "        \n",
    "        height_ratio = img_h_post/img_h_pre\n",
    "        width_ratio = img_w_post/img_w_pre\n",
    "                \n",
    "        box[:,0] = box[:,0]*width_ratio\n",
    "        box[:,1] = box[:,1]*width_ratio\n",
    "        box[:,2] = box[:,2]*height_ratio\n",
    "        box[:,3] = box[:,3]*height_ratio\n",
    "                        \n",
    "        boxes = (box).tolist()\n",
    "        \n",
    "        \n",
    "        return image, boxes, label    \n",
    "        \n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1dcabe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lucidrains/vit-pytorch/issues/159\n",
    "\n",
    "inference_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_dim, image_dim)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "   std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "\n",
    "dataset = Pascal_Dataset(data_folder, inference_transform)\n",
    "loader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "6b556c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given input image, draw rectangles as specified by gt_box and pred_box and display\n",
    "def visualize_tensor(img, gt_box, pred_box,save_image='',tb_writer=None):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    transform_img = inv_normalize(img[0]).permute(1,2,0).to('cpu').numpy()\n",
    "    transform_img = transform_img.copy()\n",
    "    for box in gt_box:\n",
    "        x0, x1, y0, y1 = box\n",
    "        cv2.rectangle(transform_img, (int(x0),int(y0)), (int(x1),int(y1)), color=(0, 255, 255), thickness=2)\n",
    "    for box in pred_box:\n",
    "        x0, x1, y0, y1 = box\n",
    "        cv2.rectangle(transform_img, (int(x0), int(y0)), (int(x1), int(y1)), color=(255, 0, 0), thickness=2)\n",
    "    \n",
    "    if tb_writer:\n",
    "      # grid = torchvision.utils.make_grid(transform_img)\n",
    "      tb_writer.add_image(save_image, transform_img, dataformats='HWC')\n",
    "    elif save_image == '':\n",
    "        plt.imshow(transform_img)\n",
    "        plt.show()  \n",
    "    else:\n",
    "        plt.imshow(transform_img)\n",
    "        plt.savefig(save_image + '.png')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "8e235384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference - https://medium.com/@fractal.ai/guide-to-build-faster-rcnn-in-pytorch-42d47cb0ecd3\n",
    "\n",
    "# Generate anchor for point\n",
    "def gen_anchor_for_pt(x, y):\n",
    "    anchor_boxes = torch.zeros((len(anchor_scale) * len(anchor_ratio),4))\n",
    "    x_center = (x*2+1)*(conversion_scale/2) \n",
    "    y_center = (y*2+1)*(conversion_scale/2)\n",
    "    for ratio_idx, ratio in enumerate(anchor_ratio):\n",
    "        for scale_idx, scale in enumerate(anchor_scale):\n",
    "            i = len(anchor_scale)*ratio_idx + scale_idx\n",
    "            h = conversion_scale*scale*torch.sqrt(ratio)\n",
    "            w = conversion_scale*scale*torch.sqrt(1.0/ratio)\n",
    "            \n",
    "            anchor_boxes[i,0] = x_center - w/2\n",
    "            anchor_boxes[i,1] = x_center + w/2\n",
    "            anchor_boxes[i,2] = y_center - h/2\n",
    "            anchor_boxes[i,3] = y_center + h/2\n",
    "            \n",
    "    return anchor_boxes\n",
    "\n",
    "# Generate anchors for entire image \n",
    "def gen_anchors(width, height):\n",
    "    anchor_boxes = torch.zeros((width*height, len(anchor_scale) * len(anchor_ratio), 4))\n",
    "    for x_center in range(width):\n",
    "        for y_center in range(height):\n",
    "            i = x_center*height + y_center\n",
    "            anchors = gen_anchor_for_pt(x_center, y_center)\n",
    "            anchor_boxes[i] = anchors\n",
    "    return anchor_boxes.reshape(-1,4)\n",
    "\n",
    "# Remove anchor outside image boundings\n",
    "def get_valid_anchors(anchor_boxes):\n",
    "    valid_anchors = torch.where((anchor_boxes[:,0] >= 0) & \n",
    "             (anchor_boxes[:,1] <= image_dim) &\n",
    "             (anchor_boxes[:,2] >= 0) &\n",
    "             (anchor_boxes[:,3] <= image_dim))[0]\n",
    "\n",
    "    anchor_boxes = anchor_boxes[valid_anchors]\n",
    "    return anchor_boxes, valid_anchors\n",
    "    \n",
    "                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "131854db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Intersection over Union between all the boxes in valid vs ground truth boxes\n",
    "def get_iou(valid_boxes, gt_boxes):\n",
    "    iou_matrix = torch.zeros((len(valid_boxes), len(gt_boxes)))\n",
    "    for i , gt_box in enumerate(gt_boxes):\n",
    "        if isinstance(gt_box,torch.Tensor):\n",
    "            gt = torch.cat([gt_box]*len(valid_boxes)).view(1,-1,4)[0]\n",
    "        else:\n",
    "            gt = torch.FloatTensor([gt_box]*len(valid_boxes))\n",
    "        xmax = torch.max(gt[:,0],valid_boxes[:,0])\n",
    "        xmin = torch.min(gt[:,1],valid_boxes[:,1])\n",
    "        ymax = torch.max(gt[:,2],valid_boxes[:,2])\n",
    "        ymin = torch.min(gt[:,3],valid_boxes[:,3])\n",
    "                \n",
    "        invalid_roi = (xmin < xmax) | (ymin < ymax)\n",
    "        roi_area = (xmin - xmax)*(ymin - ymax)\n",
    "        roi_area[invalid_roi] = 0\n",
    "        \n",
    "        total_area = (gt[:,1] - gt[:,0])*(gt[:,3] - gt[:,2]) + \\\n",
    "        (valid_boxes[:,1] - valid_boxes[:,0])*(valid_boxes[:,3]-valid_boxes[:,2]) - roi_area\n",
    "                    \n",
    "        iou = roi_area/(total_area + 1e-6)     \n",
    "        iou_matrix[:,i] = iou\n",
    "    return iou_matrix\n",
    "        \n",
    "\n",
    "def get_max_iou(iou_matrix):\n",
    "    gt_max_value = iou_matrix.max(axis=0)[0]\n",
    "    all_gt_max = torch.where(iou_matrix == gt_max_value)[0]\n",
    "    \n",
    "    anchor_max_value = torch.max(iou_matrix, axis=1)[0]\n",
    "    anchor_max = torch.argmax(iou_matrix, axis=1)\n",
    "    \n",
    "    return all_gt_max, anchor_max_value, anchor_max\n",
    "\n",
    "\n",
    "\n",
    "# positive - 1, negative - 0, ignore - -1\n",
    "def get_anchor_labels(anchor_boxes, all_gt_max, anchor_max_value):\n",
    "    anchor_labels = torch.zeros(anchor_boxes.shape[0])\n",
    "    anchor_labels.fill_(-1.0)\n",
    "    anchor_labels[anchor_max_value < 0.3] = 0\n",
    "    anchor_labels[all_gt_max] = 1.0\n",
    "    anchor_labels[anchor_max_value > 0.7] = 1.0\n",
    "    return anchor_labels\n",
    "\n",
    "\n",
    "def sample_anchors_for_train(anchor_labels):\n",
    "    sample_size = 256\n",
    "    pos_anchor = torch.where(anchor_labels == 1)[0]\n",
    "    num_pos = int(min(sample_size/2, len(pos_anchor)))\n",
    "    pos_idx = np.random.choice(pos_anchor,  num_pos, replace=False)\n",
    "\n",
    "    neg_anchor = torch.where(anchor_labels == 0)[0]\n",
    "    num_neg = sample_size - num_pos\n",
    "    neg_idx = np.random.choice(neg_anchor, int(num_neg), replace=False)\n",
    "    \n",
    "    anchor_labels[:] = -1\n",
    "    anchor_labels[pos_idx] = 1\n",
    "    anchor_labels[neg_idx] = 0\n",
    "    \n",
    "    return anchor_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5f064197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_gt(anchor_boxes, gt_boxes, anchor_max):\n",
    "    max_iou_bbox =  gt_boxes[anchor_max]\n",
    "    \n",
    "    anchor_width =  anchor_boxes[:,1] - anchor_boxes[:,0]\n",
    "    anchor_height = anchor_boxes[:,3] - anchor_boxes[:,2]\n",
    "    anchor_xctr = anchor_boxes[:,0] + anchor_width/2\n",
    "    anchor_yctr = anchor_boxes[:,2] + anchor_height/2\n",
    "    \n",
    "    gt_width =  max_iou_bbox[:,1] - max_iou_bbox[:,0]\n",
    "    gt_height = max_iou_bbox[:,3] - max_iou_bbox[:,2]\n",
    "    gt_xctr = max_iou_bbox[:,0] + gt_width/2\n",
    "    gt_yctr = max_iou_bbox[:,2] + gt_height/2\n",
    "    \n",
    "    result = torch.zeros_like(anchor_boxes)\n",
    "    result[:,0] = (gt_xctr - anchor_xctr)/anchor_width\n",
    "    result[:,1] = (gt_yctr - anchor_yctr)/anchor_height\n",
    "    result[:,2] = torch.log(gt_width/anchor_width)\n",
    "    result[:,3] = torch.log(gt_height/anchor_height)\n",
    "   \n",
    "    return result\n",
    "\n",
    "def correct_anchors(anchor_boxes, t):\n",
    "    anchor_w =  anchor_boxes[:,1] - anchor_boxes[:,0]\n",
    "    anchor_h = anchor_boxes[:,3] - anchor_boxes[:,2]\n",
    "    anchor_x = anchor_boxes[:,0] + anchor_w/2\n",
    "    anchor_y = anchor_boxes[:,2] + anchor_h/2\n",
    "\n",
    "    x = t[:,0]*anchor_w + anchor_x\n",
    "    y = t[:,1]*anchor_h + anchor_y\n",
    "    w = torch.exp(t[:,2])*anchor_w\n",
    "    h = torch.exp(t[:,3])*anchor_h\n",
    "    \n",
    "    correct_anchor = torch.zeros_like(anchor_boxes)\n",
    "    \n",
    "    correct_anchor[:,0] = x - w/2\n",
    "    correct_anchor[:,1] = x + w/2\n",
    "    correct_anchor[:,2] = y - h/2\n",
    "    correct_anchor[:,3] = y + h/2\n",
    "    \n",
    "    return correct_anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "fc37472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(anchor_boxes, class_score, img_h, img_w, is_train):\n",
    "    \n",
    "    if is_train:\n",
    "        nms_pre = 12000\n",
    "        nms_post = 2000\n",
    "    else:\n",
    "        nms_pre = 6000\n",
    "        nms_post = 300\n",
    "        \n",
    "    anchor_boxes[anchor_boxes[:,0] < 0, 0] = 0\n",
    "    anchor_boxes[anchor_boxes[:,1] > img_w, 1] = img_w\n",
    "    anchor_boxes[anchor_boxes[:,2] < 0, 2] = 0 \n",
    "    anchor_boxes[anchor_boxes[:,3] > img_h, 3] = img_h\n",
    "\n",
    "    anchor_width = anchor_boxes[:,1] - anchor_boxes[:,0]\n",
    "    anchor_height = anchor_boxes[:,3] - anchor_boxes[:,2]\n",
    "    \n",
    "    # Remove anchors boxes small than min dimensions 16, 16\n",
    "    legal_idx = (anchor_height > 16) & (anchor_width > 16)\n",
    "    anchor_boxes = anchor_boxes[legal_idx]\n",
    "    class_score = class_score[legal_idx]\n",
    "    \n",
    "    # Get top nms_pre boxes\n",
    "    sorted_class_scores = torch.argsort(class_score, descending=True)\n",
    "    top_nms_idx = sorted_class_scores[:nms_pre]\n",
    "    anchor_boxes = anchor_boxes[top_nms_idx]\n",
    "    class_score = class_score[top_nms_idx]\n",
    "    \n",
    "    # nms function expects (x1, y1, x2, y2)\n",
    "    cloned_boxes = torch.zeros_like(anchor_boxes)\n",
    "    cloned_boxes[:,0] = anchor_boxes[:,0]\n",
    "    cloned_boxes[:,1] = anchor_boxes[:,2]\n",
    "    cloned_boxes[:,2] = anchor_boxes[:,1]\n",
    "    cloned_boxes[:,3] = anchor_boxes[:,3]\n",
    "    \n",
    "    anchor_idx = nms(cloned_boxes.to('cpu'), class_score.clone().detach().to('cpu'), 0.7)[:nms_post]\n",
    "    anchor_boxes = anchor_boxes[anchor_idx]   \n",
    "    return anchor_boxes\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "6d1a279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_anchors(roi, gt_boxes, gt_labels, is_train):\n",
    "    num_sample = 128  #Number of samples to sample from roi\n",
    "\n",
    "    iou = get_iou(roi, gt_boxes)\n",
    "    roi_argmax = iou.argmax(axis=1)\n",
    "    roi_max = iou.max(axis=1)[0]\n",
    "    assign_labels = gt_labels[roi_argmax]\n",
    "    \n",
    "    num_pos = num_sample*0.25\n",
    "    pos_idx = torch.where(roi_max > 0.5)[0]\n",
    "    if is_train:\n",
    "        pos_idx = np.random.choice(pos_idx, int(min(len(pos_idx), num_pos)), replace=False)  \n",
    "    num_neg = num_sample-len(pos_idx)\n",
    "    neg_idx = torch.where(roi_max < 0.5)[0]\n",
    "    if is_train:\n",
    "        neg_idx = np.random.choice(neg_idx, int(min(len(neg_idx), num_neg)), replace=False)\n",
    "        \n",
    "    final_idx = np.append(pos_idx, neg_idx)\n",
    "    assign_labels[neg_idx] = 0\n",
    "    assign_labels = assign_labels[final_idx]\n",
    "    roi = roi[final_idx]\n",
    "    roi_argmax = roi_argmax[final_idx]\n",
    "    return assign_labels, roi, roi_argmax, final_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "f7fcaff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roi_pooling(roi, features):\n",
    "    layer = nn.AdaptiveMaxPool2d((7,7)).to(device)\n",
    "    roi = roi/16.0\n",
    "    result = []\n",
    "    for i in roi:\n",
    "        x1 = int(i[0])\n",
    "        x2 = int(i[1]+1)\n",
    "        y1 = int(i[2])\n",
    "        y2 = int(i[3]+1)\n",
    "        result.append(layer(features[:,:,y1:y2,x1:x2]))\n",
    "    result = torch.cat(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "67694c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_Backbone(): \n",
    "    vgg = models.vgg16(weights='IMAGENET1K_V1')\n",
    "    vgg_features  = vgg.features[:-1]\n",
    "    vgg_classifier = vgg.classifier[:-1]\n",
    "    \n",
    "    for layer in vgg_features[:10]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return vgg_features.to(device), vgg_classifier.to(device)\n",
    "\n",
    "class FastRCNN(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.backbone = backbone\n",
    "        self.classify = nn.Linear(4096, num_classes)\n",
    "        self.reg = nn.Linear(4096, num_classes*4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(self.flatten(x))\n",
    "        pred_class = self.classify(x)\n",
    "        pred_box = self.reg(x)\n",
    "        return pred_class, pred_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "2f073efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(self, backbone, in_features=512, hidden_dim=512, p_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.conv = nn.Conv2d(in_features, hidden_dim, 3, 1, 1)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.conf_head = nn.Conv2d(hidden_dim, 2*len(anchor_scale)*len(anchor_ratio), 1, 1, 0)\n",
    "        self.reg_head = nn.Conv2d(hidden_dim, 4*len(anchor_scale)*len(anchor_ratio), 1 ,1 , 0)\n",
    "        \n",
    "        self.conv.bias.data.zero_()\n",
    "        self.conv.weight.data.normal_(0, 0.01)\n",
    "        self.conf_head.bias.data.zero_()\n",
    "        self.conf_head.weight.data.normal_(0, 0.01)\n",
    "        self.reg_head.bias.data.zero_()\n",
    "        self.reg_head.weight.data.normal_(0, 0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        conv_out = F.relu(self.dropout(self.conv(features)))\n",
    "        conf = self.conf_head(conv_out)\n",
    "        reg = self.reg_head(conv_out)\n",
    "        return features, conf, reg\n",
    "\n",
    "def region_prop_loss(pred_box, pred_class, gt_box, gt_class):\n",
    "    gt_box = gt_box.to(device)\n",
    "    gt_class = gt_class.long().to(device)\n",
    "    \n",
    "    foreground_class_idx = (gt_class > 0)\n",
    "    pred_box = pred_box[foreground_class_idx]\n",
    "    gt_box = gt_box[foreground_class_idx]\n",
    "    reg_loss = F.smooth_l1_loss(pred_box, gt_box) / (sum(foreground_class_idx)+1e-6)\n",
    "    class_loss = F.cross_entropy(pred_class, gt_class, ignore_index=-1)\n",
    "    return reg_loss + class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "4a9c1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extracter, classifier = VGG_Backbone()\n",
    "rpn = RegionProposalNetwork(feature_extracter).to(device)\n",
    "fast_rcnn = FastRCNN(classifier).to(device)\n",
    "all_params = list(list(rpn.parameters()) + list(fast_rcnn.parameters()))\n",
    "optimizer = optim.Adam(all_params, lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "31038e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anchors = gen_anchors(int(image_dim/conversion_scale), int(image_dim/conversion_scale)).to(device)\n",
    "\n",
    "def single_image_pipeline(input_image, gt_box, label, is_train):\n",
    "\n",
    "    input_image = input_image.to(device)\n",
    "    gt_box = torch.FloatTensor(gt_box).to(device)\n",
    "    label = torch.FloatTensor(label).to(device)\n",
    "\n",
    "    valid_anchors, valid_anchors_idx = get_valid_anchors(all_anchors.clone())\n",
    "    iou = get_iou(valid_anchors, gt_box)\n",
    "    all_gt_max, anchor_max_value, anchor_max = get_max_iou(iou)\n",
    "    \n",
    "    anchor_labels = get_anchor_labels(valid_anchors, all_gt_max, anchor_max_value)\n",
    "    anchor_labels = sample_anchors_for_train(anchor_labels)\n",
    "    \n",
    "    delta = anchor_gt(valid_anchors, gt_box, anchor_max)\n",
    "    gt_boxes = torch.zeros_like(all_anchors)\n",
    "    gt_boxes[valid_anchors_idx] = delta\n",
    "\n",
    "    gt_class = torch.zeros(all_anchors.shape[0])\n",
    "    gt_class.fill_(-1)\n",
    "    gt_class[valid_anchors_idx] = anchor_labels\n",
    "    \n",
    "    features, class_out, pred_box = rpn(input_image)\n",
    "\n",
    "    pred_class = class_out.permute(0,2,3,1).reshape(1,-1,2)[0]\n",
    "    class_score = class_out.reshape(1,features.shape[2],features.shape[3],9,2)[:,:,:,:,1].reshape(1,-1)[0].detach()\n",
    "    pred_box = pred_box.permute(0,2,3,1).reshape(1,-1,4)[0]\n",
    "\n",
    "    rpn_loss = region_prop_loss(pred_box, pred_class, gt_boxes, gt_class).to(device)    \n",
    "    corrected_anchors = correct_anchors(all_anchors.to(device), pred_box).detach()\n",
    "    \n",
    "    roi = non_max_suppression(corrected_anchors, class_score, input_image.shape[2], input_image.shape[3], is_train)\n",
    "    \n",
    "    assign_labels, roi_samples, roi_argmax, idx = classify_anchors(roi, gt_box, label, is_train)\n",
    "    decider_box = anchor_gt(roi_samples, gt_box, roi_argmax)\n",
    "   \n",
    "    pooled_features = roi_pooling(roi_samples, features)\n",
    "\n",
    "    final_class, final_box = fast_rcnn(pooled_features)\n",
    "    final_box = final_box.reshape(pooled_features.shape[0],-1,4)\n",
    "    final_box = final_box[torch.arange(0,pooled_features.shape[0]), assign_labels.long()]\n",
    "    \n",
    "    rcnn_loss = region_prop_loss(final_box, final_class, decider_box, assign_labels).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        final_box_no_grad = final_box.clone().to(device)\n",
    "\n",
    "    corrected_roi = correct_anchors(roi_samples, final_box_no_grad).detach()    \n",
    "    return rpn_loss, rcnn_loss , final_class, corrected_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "94473977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(input_image):\n",
    "    input_image = input_image.to(device)\n",
    "    rpn.eval()\n",
    "    fast_rcnn.eval()\n",
    "    \n",
    "    features, class_out, pred_box = rpn(input_image)\n",
    "    pred_box = pred_box.permute(0,2,3,1).reshape(1,-1,4)[0]\n",
    "    class_score = class_out.reshape(1,features.shape[2],features.shape[3],9,2)[:,:,:,:,1].reshape(1,-1)[0].detach()\n",
    "\n",
    "    corrected_anchors = correct_anchors(all_anchors.to(device), pred_box).detach()\n",
    "    roi = non_max_suppression(corrected_anchors, class_score, input_image.shape[2],input_image.shape[3], False)\n",
    "    pooled_features = roi_pooling(roi, features)\n",
    "\n",
    "    final_class, final_box = fast_rcnn(pooled_features)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        final_box_no_grad = final_box.clone().to(device)\n",
    "\n",
    "    corrected_roi = correct_anchors(roi, final_box_no_grad).detach()\n",
    "    \n",
    "    \"\"\"\n",
    "    corrected_roi[corrected_roi[:,0] < 0,0] = 0 # x1\n",
    "    corrected_roi[corrected_roi[:,1] > input_image_width,1] = input_image_width # x2\n",
    "    corrected_roi[corrected_roi[:,2] < 0,2] = 0 # y1\n",
    "    corrected_roi[corrected_roi[:,3] > input_image_height,3] = input_image_height # y2\n",
    "    \"\"\"\n",
    "    \n",
    "    print(corrected_roi)\n",
    "\n",
    "    final_conf = final_class.softmax(dim=1).max(dim=1)[0]\n",
    "    final_conf = final_conf.detach()\n",
    "    final_conf = final_conf[final_class.argmax(axis=1) != 0]\n",
    "    sorted_class_scores = torch.argsort(final_conf, descending=True)\n",
    "\n",
    "    keep_anchors = []\n",
    "    while len(sorted_class_scores) > 1:\n",
    "        current = sorted_class_scores[0]\n",
    "        keep_anchors.append(current.item())\n",
    "        iou_matrix = get_iou(corrected_roi[sorted_class_scores[1:]],corrected_roi[current].reshape(1,-1,4)[0])\n",
    "        sorted_class_scores = sorted_class_scores[np.where(iou_matrix < 0.2)[0] + 1]\n",
    "\n",
    "    if (len(sorted_class_scores) == 1):\n",
    "        keep_anchors.append(sorted_class_scores[0].item())\n",
    "\n",
    "    for pred in final_class.argmax(axis=1)[final_class.argmax(axis=1) != 0][keep_anchors]:\n",
    "        print(id_to_label[int(pred)], end=', ')\n",
    "    print('')\n",
    "\n",
    "    for pred in final_conf[keep_anchors]:\n",
    "        print(pred.item(), end=', ')\n",
    "\n",
    "    visualize_tensor(input_image, roi[final_class.argmax(axis=1) != 0][keep_anchors], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "93ca460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Average loss for 1 training images = tensor(2.8229)\n",
      "Average loss for 1 training images = tensor(2.3050)\n",
      "Average loss for 1 training images = tensor(1.9373)\n",
      "Average loss for 1 training images = tensor(1.5567)\n",
      "Average loss for 1 training images = tensor(1.9887)\n",
      "Average loss for 1 training images = tensor(1.5760)\n",
      "Average loss for 1 training images = tensor(1.9235)\n",
      "Average loss for 1 training images = tensor(2.1172)\n",
      "Average loss for 1 training images = tensor(2.2310)\n",
      "Average loss for 1 training images = tensor(1.2618)\n",
      "Average loss for 1 training images = tensor(1.4333)\n",
      "Average loss for 1 training images = tensor(1.3523)\n",
      "Average loss for 1 training images = tensor(1.4916)\n",
      "Average loss for 1 training images = tensor(1.2429)\n",
      "Average loss for 1 training images = tensor(1.3665)\n",
      "Average loss for 1 training images = tensor(0.9865)\n",
      "Average loss for 1 training images = tensor(1.4124)\n",
      "Average loss for 1 training images = tensor(0.9198)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[289], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      9\u001b[0m     img, gt_box, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m---> 10\u001b[0m     loss1, loss2, pred_class, pred_box \u001b[38;5;241m=\u001b[39m \u001b[43msingle_image_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     net_loss \u001b[38;5;241m=\u001b[39m loss1 \u001b[38;5;241m+\u001b[39m loss2\n\u001b[1;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[287], line 30\u001b[0m, in \u001b[0;36msingle_image_pipeline\u001b[0;34m(input_image, gt_box, label, is_train)\u001b[0m\n\u001b[1;32m     27\u001b[0m class_score \u001b[38;5;241m=\u001b[39m class_out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m],\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m2\u001b[39m)[:,:,:,:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     28\u001b[0m pred_box \u001b[38;5;241m=\u001b[39m pred_box\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m rpn_loss \u001b[38;5;241m=\u001b[39m \u001b[43mregion_prop_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_class\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)    \n\u001b[1;32m     31\u001b[0m corrected_anchors \u001b[38;5;241m=\u001b[39m correct_anchors(all_anchors\u001b[38;5;241m.\u001b[39mto(device), pred_box)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     33\u001b[0m roi \u001b[38;5;241m=\u001b[39m non_max_suppression(corrected_anchors, class_score, input_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], input_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m], is_train)\n",
      "Cell \u001b[0;32mIn[285], line 31\u001b[0m, in \u001b[0;36mregion_prop_loss\u001b[0;34m(pred_box, pred_class, gt_box, gt_class)\u001b[0m\n\u001b[1;32m     29\u001b[0m pred_box \u001b[38;5;241m=\u001b[39m pred_box[foreground_class_idx]\n\u001b[1;32m     30\u001b[0m gt_box \u001b[38;5;241m=\u001b[39m gt_box[foreground_class_idx]\n\u001b[0;32m---> 31\u001b[0m reg_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msmooth_l1_loss(pred_box, gt_box) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mforeground_class_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m     32\u001b[0m class_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(pred_class, gt_class, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reg_loss \u001b[38;5;241m+\u001b[39m class_loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "counter = 0\n",
    "print_net_loss = 0\n",
    "print_step = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Starting epoch \" + str(epoch))\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        img, gt_box, labels = data\n",
    "        loss1, loss2, pred_class, pred_box = single_image_pipeline(img.to(device), gt_box, labels, True)\n",
    "        net_loss = loss1 + loss2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        net_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        net_loss = net_loss.detach()\n",
    "        if not(math.isnan(net_loss)):\n",
    "            counter += 1\n",
    "            print_net_loss += net_loss.data\n",
    "      \n",
    "        if (counter % print_step == 0):\n",
    "            print(\"Average loss for \" + str(counter) + \" training images = \" + str(print_net_loss/(counter + 1e-6)))\n",
    "            counter = 0\n",
    "            print_net_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae20f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('/Users/shivneshwarvelayutham/Downloads/fasterrcnn/006585.jpg')\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "test_model(img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
