{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sTsDfIVKsmL"
   },
   "source": [
    "# DAT405 Introduction to Data Science and AI - Shivneshwar Velayutham (Chalmers)\n",
    "## 2022-2023, Reading Period 2\n",
    "## Assignment 4: Spam classification using Naïve Bayes \n",
    "There will be an overall grade for this assignment. To get a pass grade (grade 5), you need to pass items 1-3 below. To receive higher grades, finish items 4 and 5 as well. \n",
    "\n",
    "The exercise takes place in a notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \n",
    "Hints:\n",
    "You can execute certain linux shell commands by prefixing the command with `!`. You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results the second you can use writing code snippets that execute the tasks required.  \n",
    "\n",
    "In this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into spam and non-spam (“ham”) classes.  Your program should be able to train on a given set of spam and “ham” datasets. \n",
    "You will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. There are three types of files in this location: \n",
    "-\teasy-ham: non-spam messages typically quite easy to differentiate from spam messages. \n",
    "-\thard-ham: non-spam messages more difficult to differentiate \n",
    "-\tspam: spam messages \n",
    "\n",
    "**Execute the cell below to download and extract the data into the environment of the notebook -- it will take a few seconds.** If you chose to use Jupyter notebooks you will have to run the commands in the cell below on your local computer, with Windows you can use \n",
    "7zip (https://www.7-zip.org/download.html) to decompress the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "Wa37fBwRF-xe",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-29 23:51:47--  https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1677144 (1.6M) [application/x-bzip2]\n",
      "Saving to: ‘20021010_easy_ham.tar.bz2.8’\n",
      "\n",
      "20021010_easy_ham.t 100%[===================>]   1.60M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2022-11-29 23:51:48 (18.5 MB/s) - ‘20021010_easy_ham.tar.bz2.8’ saved [1677144/1677144]\n",
      "\n",
      "--2022-11-29 23:51:48--  https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1021126 (997K) [application/x-bzip2]\n",
      "Saving to: ‘20021010_hard_ham.tar.bz2.8’\n",
      "\n",
      "20021010_hard_ham.t 100%[===================>] 997.19K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2022-11-29 23:51:48 (13.4 MB/s) - ‘20021010_hard_ham.tar.bz2.8’ saved [1021126/1021126]\n",
      "\n",
      "--2022-11-29 23:51:48--  https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1192582 (1.1M) [application/x-bzip2]\n",
      "Saving to: ‘20021010_spam.tar.bz2.8’\n",
      "\n",
      "20021010_spam.tar.b 100%[===================>]   1.14M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2022-11-29 23:51:48 (16.0 MB/s) - ‘20021010_spam.tar.bz2.8’ saved [1192582/1192582]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Download and extract data\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
    "!tar -xjf 20021010_easy_ham.tar.bz2\n",
    "!tar -xjf 20021010_hard_ham.tar.bz2\n",
    "!tar -xjf 20021010_spam.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdH1XTepLjZ3"
   },
   "source": [
    "*The* data is now in the three folders `easy_ham`, `hard_ham`, and `spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "A53Gw00fBLG2",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 68632\r\n",
      "drwxr-xr-x    35 shivneshwarvelayutham  staff   1.1K Nov 29 23:51 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x     9 shivneshwarvelayutham  staff   288B Nov 29 00:20 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--@    1 shivneshwarvelayutham  staff   6.0K Nov 29 22:06 .DS_Store\r\n",
      "drwxr-xr-x     3 shivneshwarvelayutham  staff    96B Nov 29 15:12 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.6M Jun 29  2004 20021010_easy_ham.tar.bz2\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.1\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.2\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.3\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.4\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.5\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.6\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.7\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.8\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   997K Dec 16  2004 20021010_hard_ham.tar.bz2\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   997K Dec 16  2004 20021010_hard_ham.tar.bz2.1\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   997K Dec 16  2004 20021010_hard_ham.tar.bz2.2\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   997K Dec 16  2004 20021010_hard_ham.tar.bz2.3\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   997K Dec 16  2004 20021010_hard_ham.tar.bz2.4\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   997K Dec 16  2004 20021010_hard_ham.tar.bz2.5\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   997K Dec 16  2004 20021010_hard_ham.tar.bz2.6\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   997K Dec 16  2004 20021010_hard_ham.tar.bz2.7\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   997K Dec 16  2004 20021010_hard_ham.tar.bz2.8\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.1M Jun 29  2004 20021010_spam.tar.bz2\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.1M Jun 29  2004 20021010_spam.tar.bz2.1\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.1M Jun 29  2004 20021010_spam.tar.bz2.2\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.1M Jun 29  2004 20021010_spam.tar.bz2.3\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.1M Jun 29  2004 20021010_spam.tar.bz2.4\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.1M Jun 29  2004 20021010_spam.tar.bz2.5\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.1M Jun 29  2004 20021010_spam.tar.bz2.6\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.1M Jun 29  2004 20021010_spam.tar.bz2.7\r\n",
      "-rw-r--r--     1 shivneshwarvelayutham  staff   1.1M Jun 29  2004 20021010_spam.tar.bz2.8\r\n",
      "-rw-r--r--@    1 shivneshwarvelayutham  staff    32K Nov 29 23:51 Assignment4-1.ipynb\r\n",
      "drwx--x--x  2553 shivneshwarvelayutham  staff    80K Nov 29 23:51 \u001b[34measy_ham\u001b[m\u001b[m\r\n",
      "drwx--x--x   252 shivneshwarvelayutham  staff   7.9K Nov 29 23:51 \u001b[34mhard_ham\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   503 shivneshwarvelayutham  staff    16K Nov 29 23:51 \u001b[34mspam\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGlWPVnSNzT7"
   },
   "source": [
    "###1. Preprocessing: \n",
    "1.\tNote that the email files contain a lot of extra information, besides the actual message. Ignore that for now and run on the entire text. Further down (in the higher-grade part), you will be asked to filter out the headers and footers. \n",
    "2.\tWe don’t want to train and test on the same data. Split the spam and the ham datasets in a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "J2sllUWXKblD",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import email\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = []\n",
    "easy_ham = []\n",
    "hard_ham = []\n",
    "\n",
    "spam_files = os.listdir('spam')\n",
    "for spam_file in spam_files:\n",
    "    with open(os.path.join('spam', spam_file), encoding=\"latin-1\") as f:\n",
    "        spam.append(f.read())\n",
    "\n",
    "ham_files = os.listdir('easy_ham')\n",
    "for ham_file in ham_files:\n",
    "    with open(os.path.join('easy_ham', ham_file), encoding=\"latin-1\") as f:\n",
    "        easy_ham.append(f.read())\n",
    "\n",
    "ham_files = os.listdir('hard_ham')\n",
    "for ham_file in ham_files:\n",
    "    with open(os.path.join('hard_ham', ham_file), encoding=\"latin-1\") as f:\n",
    "        hard_ham.append(f.read()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_train(df, vectorizer, classifiers):\n",
    "    X=df[\"Text\"]\n",
    "    y=df[\"Label\"]\n",
    "    \n",
    "    print(\"\\nNumber of spam/ham in entire set\")\n",
    "    print(y.value_counts())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True , stratify=y)\n",
    "    print(\"\\nNumber of spam/ham in training set\")\n",
    "    print(y_train.value_counts())\n",
    "    \n",
    "    for classifier in classifiers:\n",
    "        print(\"\\nTrying \" + str(classifier))\n",
    "        pipe = Pipeline(steps=[('vectorize', vectorizer), ('classifier', classifier)])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_predict = pipe.predict(X_test)\n",
    "\n",
    "        print(\"Accuracy score: \" + str(accuracy_score(y_test, y_predict)))\n",
    "        print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnbrbI0_OKCF"
   },
   "source": [
    "###2. Write a Python program that: \n",
    "1.\tUses four datasets (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) \n",
    "2.\tTrains a Naïve Bayes classifier (e.g. Sklearn) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and False Negative rates on the `hamtest` and `spamtest` datasets. You can use `CountVectorizer` to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifier in SKlearn ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Test two of these classifiers that are well suited for this problem\n",
    "- Multinomial Naive Bayes  \n",
    "- Bernoulli Naive Bayes. \n",
    "\n",
    "Please inspect the documentation to ensure input to the classifiers is appropriate. Discuss the differences between these two classifiers. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with both easy and hard ham\n",
      "\n",
      "Number of spam/ham in entire set\n",
      "ham     2801\n",
      "spam     501\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Number of spam/ham in training set\n",
      "ham     2240\n",
      "spam     401\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Trying MultinomialNB()\n",
      "Accuracy score: 0.9773071104387292\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.99      0.99       561\n",
      "        spam       0.97      0.88      0.92       100\n",
      "\n",
      "    accuracy                           0.98       661\n",
      "   macro avg       0.97      0.94      0.95       661\n",
      "weighted avg       0.98      0.98      0.98       661\n",
      "\n",
      "\n",
      "Trying BernoulliNB()\n",
      "Accuracy score: 0.8910741301059002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.89      1.00      0.94       561\n",
      "        spam       1.00      0.28      0.44       100\n",
      "\n",
      "    accuracy                           0.89       661\n",
      "   macro avg       0.94      0.64      0.69       661\n",
      "weighted avg       0.90      0.89      0.86       661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training with both easy and hard ham\")\n",
    "df1 = pd.DataFrame (spam, columns = ['Text'])\n",
    "df1['Label'] = 'spam'\n",
    "\n",
    "df2 = pd.DataFrame (easy_ham + hard_ham, columns = ['Text'])\n",
    "df2['Label'] = 'ham'\n",
    "\n",
    "split_and_train(pd.concat([df1, df2]), CountVectorizer(strip_accents='ascii', encoding=\"latin-1\"), [MultinomialNB(), BernoulliNB()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI1bPDCvQxen"
   },
   "source": [
    "Your discussion here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDFS3uFFUcS7"
   },
   "source": [
    "### 3.Run your program on \n",
    "-\tSpam versus easy-ham \n",
    "-\tSpam versus hard-ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "gool_zb8Qzzy",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with only easy ham\n",
      "\n",
      "Number of spam/ham in entire set\n",
      "ham     2551\n",
      "spam     501\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Number of spam/ham in training set\n",
      "ham     2040\n",
      "spam     401\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Trying MultinomialNB()\n",
      "Accuracy score: 0.9656301145662848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98       511\n",
      "        spam       0.99      0.80      0.88       100\n",
      "\n",
      "    accuracy                           0.97       611\n",
      "   macro avg       0.97      0.90      0.93       611\n",
      "weighted avg       0.97      0.97      0.96       611\n",
      "\n",
      "\n",
      "Trying BernoulliNB()\n",
      "Accuracy score: 0.8723404255319149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.87      0.99      0.93       511\n",
      "        spam       0.84      0.27      0.41       100\n",
      "\n",
      "    accuracy                           0.87       611\n",
      "   macro avg       0.86      0.63      0.67       611\n",
      "weighted avg       0.87      0.87      0.84       611\n",
      "\n",
      "Training with only hard ham\n",
      "\n",
      "Number of spam/ham in entire set\n",
      "spam    501\n",
      "ham     250\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Number of spam/ham in training set\n",
      "spam    400\n",
      "ham     200\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Trying MultinomialNB()\n",
      "Accuracy score: 0.9072847682119205\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.88      0.84      0.86        50\n",
      "        spam       0.92      0.94      0.93       101\n",
      "\n",
      "    accuracy                           0.91       151\n",
      "   macro avg       0.90      0.89      0.89       151\n",
      "weighted avg       0.91      0.91      0.91       151\n",
      "\n",
      "\n",
      "Trying BernoulliNB()\n",
      "Accuracy score: 0.8741721854304636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      0.64      0.77        50\n",
      "        spam       0.85      0.99      0.91       101\n",
      "\n",
      "    accuracy                           0.87       151\n",
      "   macro avg       0.91      0.82      0.84       151\n",
      "weighted avg       0.89      0.87      0.87       151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training with only easy ham\")\n",
    "df2 = pd.DataFrame (easy_ham, columns = ['Text'])\n",
    "df2['Label'] = 'ham'\n",
    "\n",
    "split_and_train(pd.concat([df1, df2]), CountVectorizer(strip_accents='ascii', encoding=\"latin-1\"), [MultinomialNB(), BernoulliNB()])\n",
    "\n",
    "print(\"Training with only hard ham\")\n",
    "df2 = pd.DataFrame (hard_ham, columns = ['Text'])\n",
    "df2['Label'] = 'ham'\n",
    "\n",
    "split_and_train(pd.concat([df1, df2]), CountVectorizer(strip_accents='ascii', encoding=\"latin-1\"), [MultinomialNB(), BernoulliNB()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkfQWBB4UhYd"
   },
   "source": [
    "###4.\tTo avoid classification based on common and uninformative words it is common to filter these out. \n",
    "\n",
    "**a.** Argue why this may be useful. Try finding the words that are too common/uncommon in the dataset. \n",
    "\n",
    "**b.** Use the parameters in Sklearn’s `CountVectorizer` to filter out these words. Update the program from point 3 and run it on your data and report your results.\n",
    "\n",
    "You have two options to do this in Sklearn: either using the words found in part (a) or letting Sklearn do it for you. Argue for your decision-making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "qt7ELzEqUfas",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('com', 69898),\n",
      " ('the', 40814),\n",
      " ('to', 38179),\n",
      " ('http', 34048),\n",
      " ('from', 28715),\n",
      " ('td', 28399),\n",
      " ('2002', 28274),\n",
      " ('3d', 25415),\n",
      " ('for', 23845),\n",
      " ('net', 22839)]\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(strip_accents='ascii', encoding=\"latin-1\").fit(spam + easy_ham + hard_ham)\n",
    "bag_of_words = vec.transform(spam + easy_ham + hard_ham)\n",
    "sum_words = bag_of_words.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "pprint(words_freq[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "### 4a)\n",
    "This would help removing words that do not have any correlation to whether an email is spam or not, hence why they are called uninformative for our specific needs. The commons words are printed above and as we can see there are words like **the and to and from** which are uninformative words for whom there is no corellation to whether the email is spam or not and so we don't need to take this words into consideration. \n",
    "\n",
    "### 4b)\n",
    "I am letting sci kit learn do it since e can use already established knowledge of words that are uninformative and can be ignored and this way a mistake of leaving certain words by mistake does not occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with both easy and hard ham and with stop words\n",
      "\n",
      "Number of spam/ham in entire set\n",
      "ham     2801\n",
      "spam     501\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Number of spam/ham in training set\n",
      "ham     2240\n",
      "spam     401\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Trying MultinomialNB()\n",
      "Accuracy score: 0.9863842662632375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99       561\n",
      "        spam       0.98      0.93      0.95       100\n",
      "\n",
      "    accuracy                           0.99       661\n",
      "   macro avg       0.98      0.96      0.97       661\n",
      "weighted avg       0.99      0.99      0.99       661\n",
      "\n",
      "\n",
      "Trying BernoulliNB()\n",
      "Accuracy score: 0.8910741301059002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.89      1.00      0.94       561\n",
      "        spam       1.00      0.28      0.44       100\n",
      "\n",
      "    accuracy                           0.89       661\n",
      "   macro avg       0.94      0.64      0.69       661\n",
      "weighted avg       0.90      0.89      0.86       661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training with both easy and hard ham and with stop words\")\n",
    "df1 = pd.DataFrame (spam, columns = ['Text'])\n",
    "df1['Label'] = 'spam'\n",
    "\n",
    "df2 = pd.DataFrame (easy_ham + hard_ham, columns = ['Text'])\n",
    "df2['Label'] = 'ham'\n",
    "\n",
    "split_and_train(pd.concat([df1, df2]), CountVectorizer(stop_words='english', strip_accents='ascii', encoding=\"latin-1\")\n",
    "                , [MultinomialNB(), BernoulliNB()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcyVfOZFU4F_"
   },
   "source": [
    "###5. Eeking out further performance\n",
    "Filter out the headers and footers of the emails before you run on them. The format may vary somewhat between emails, which can make this a bit tricky, so perfect filtering is not required. Run your program again and answer the following questions: \n",
    "-\tDoes the result improve from 3 and 4? \n",
    "- The split of the data set into a training set and a test set can lead to very skewed results. Why is this, and do you have suggestions on remedies? \n",
    "- What do you expect would happen if your training set were mostly spam messages while your test set were mostly ham messages? \n",
    "\n",
    "Re-estimate your classifier using `fit_prior` parameter set to `false`, and answer the following questions:\n",
    "- What does this parameter mean?\n",
    "- How does this alter the predictions? Discuss why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "id": "s_nyGug9U4f3",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with both easy and hard ham without headers\n",
      "\n",
      "Number of spam/ham in entire set\n",
      "ham     2801\n",
      "spam     501\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Number of spam/ham in training set\n",
      "ham     2240\n",
      "spam     401\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Trying MultinomialNB()\n",
      "Accuracy score: 0.9757942511346445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.99       561\n",
      "        spam       0.99      0.85      0.91       100\n",
      "\n",
      "    accuracy                           0.98       661\n",
      "   macro avg       0.98      0.92      0.95       661\n",
      "weighted avg       0.98      0.98      0.98       661\n",
      "\n",
      "\n",
      "Trying BernoulliNB()\n",
      "Accuracy score: 0.8835098335854765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.88      1.00      0.94       561\n",
      "        spam       0.93      0.25      0.39       100\n",
      "\n",
      "    accuracy                           0.88       661\n",
      "   macro avg       0.90      0.62      0.66       661\n",
      "weighted avg       0.89      0.88      0.85       661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam = []\n",
    "easy_ham = []\n",
    "hard_ham = []\n",
    "\n",
    "spam_files = os.listdir('spam')\n",
    "for spam_file in spam_files:\n",
    "    with open(os.path.join('spam', spam_file), encoding=\"latin-1\") as f:\n",
    "        email_message = email.message_from_file(f)\n",
    "        body = \"\"\n",
    "        if email_message.is_multipart():\n",
    "            for part in email_message.get_payload():\n",
    "                body += ''.join(map(str, part.get_payload()))\n",
    "        else:\n",
    "            body = email_message.get_payload()\n",
    "        spam.append(body)\n",
    "\n",
    "        \n",
    "ham_files = os.listdir('easy_ham')\n",
    "for ham_file in ham_files:\n",
    "    with open(os.path.join('easy_ham', ham_file), encoding=\"latin-1\") as f:\n",
    "        email_message = email.message_from_file(f)\n",
    "        body = \"\"\n",
    "        if email_message.is_multipart():\n",
    "            for part in email_message.get_payload():\n",
    "                body += ''.join(map(str, part.get_payload()))\n",
    "        else:\n",
    "            body = email_message.get_payload()\n",
    "        easy_ham.append(body)\n",
    "\n",
    "ham_files = os.listdir('hard_ham')\n",
    "for ham_file in ham_files:\n",
    "    with open(os.path.join('hard_ham', ham_file), encoding=\"latin-1\") as f:\n",
    "        email_message = email.message_from_file(f)\n",
    "        body = \"\"\n",
    "        if email_message.is_multipart():\n",
    "            for part in email_message.get_payload():\n",
    "                body += ''.join(map(str, part.get_payload()))\n",
    "        else:\n",
    "            body = email_message.get_payload()\n",
    "        hard_ham.append(body) \n",
    "\n",
    "print(\"Training with both easy and hard ham without headers\")\n",
    "\n",
    "df1 = pd.DataFrame (spam, columns = ['Text'])\n",
    "df1['Label'] = 'spam'\n",
    "\n",
    "df2 = pd.DataFrame (easy_ham + hard_ham, columns = ['Text'])\n",
    "df2['Label'] = 'ham'\n",
    "\n",
    "split_and_train(pd.concat([df1, df2]), CountVectorizer(strip_accents='ascii', encoding=\"latin-1\"), [MultinomialNB(), BernoulliNB()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "### 5a)\n",
    "There isn't in significant difference when the headers and footers are ignored and the accuracies are quite similar to before.\n",
    "\n",
    "Sometimes splitting of data can result in vastly different training set and test set. This can especially happen if we have certain classes of data (ie. data is split based on certain rules.) Thus care must be taken to ensure that the training data has all the different classification/types of data. The remedy used in this notebook is with the help of the **stratify argument of train_test_split** which is able to ensure that the training data is similarly distributed as the original data. \n",
    "\n",
    "If training data was mostly spam then the accuracy on the testing data would be quite low as it might predict all the ham messages as spam. Thus it's quite important to run the classification multiple times to check if there are big oscillations of accuracy scores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "8bI3z_spVacz",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with both easy and hard ham and fit_prior set to false\n",
      "\n",
      "Number of spam/ham in entire set\n",
      "ham     2801\n",
      "spam     501\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Number of spam/ham in training set\n",
      "ham     2240\n",
      "spam     401\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Trying MultinomialNB(fit_prior=False)\n",
      "Accuracy score: 0.9682299546142209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      0.99      0.98       561\n",
      "        spam       0.96      0.82      0.89       100\n",
      "\n",
      "    accuracy                           0.97       661\n",
      "   macro avg       0.97      0.91      0.93       661\n",
      "weighted avg       0.97      0.97      0.97       661\n",
      "\n",
      "\n",
      "Trying BernoulliNB(fit_prior=False)\n",
      "Accuracy score: 0.8714069591527988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.87      1.00      0.93       561\n",
      "        spam       0.89      0.17      0.29       100\n",
      "\n",
      "    accuracy                           0.87       661\n",
      "   macro avg       0.88      0.58      0.61       661\n",
      "weighted avg       0.87      0.87      0.83       661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training with both easy and hard ham and fit_prior set to false\")\n",
    "df1 = pd.DataFrame (spam, columns = ['Text'])\n",
    "df1['Label'] = 'spam'\n",
    "\n",
    "df2 = pd.DataFrame (easy_ham + hard_ham, columns = ['Text'])\n",
    "df2['Label'] = 'ham'\n",
    "\n",
    "split_and_train(pd.concat([df1, df2]), CountVectorizer(strip_accents='ascii', encoding=\"latin-1\"), [MultinomialNB(fit_prior=False), BernoulliNB(fit_prior=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b)\n",
    "\n",
    "The fit_prior parameter decides whether to learn prior probablities of the different classes from the data or to use uniform prior probabilties (where the probablily of each class occuring is the same). When fit_prior is set to True then it takes into account the distribution of classes in the data during prediction. In our case, as we can have more ham than spam so the model will taken this account. When fit_prior is set to False then each class has an equal probablity of occurring (in our case 0.5 to ham, 0.5 to spam). This will have an impact in predictions since if the data contains a lot of examples of a specific class and fit_prior is set to True then it will be more likely that the model predicts the class which has a lot of examples in the data. This does not occur and each class is treated equally when fit_prior is set to False. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
