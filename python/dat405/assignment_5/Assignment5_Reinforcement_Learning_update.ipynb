{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3D82waLqItO"
   },
   "source": [
    "#DAT405 Introduction to Data Science and AI - Shivneshwar Velayutham\n",
    "##2022-2023, Reading Period 1\n",
    "## Assignment 5: Reinforcement learning and classification\n",
    "\n",
    "The exercise takes place in a notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \n",
    "Hints:\n",
    "You can execute certain linux shell commands by prefixing the command with `!`. You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results the second you can use writing code snippets that execute the tasks required.  \n",
    "\n",
    "This assignment is about **sequential decision making** under uncertainty (Reinforcement learning). In a sequential decision process, the process jumps between different states (the environment), and in each state the decision maker, or agent, chooses among a set of actions. Given the state and the chosen action, the process jumps to a new state. At each jump the decision maker receives a reward, and the objective is to find a sequence of decisions (or an optimal policy) that maximizes the accumulated rewards.\n",
    "\n",
    "We will use **Markov decision processes** (MDPs) to model the environment, and below is a primer on the relevant background theory. The assignment can be divided in two parts:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiO_zpY7qItS"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUyGq4olqItS"
   },
   "source": [
    "The first question covers a deterministic MPD, where the action is directly given by the state, described as follows:\n",
    "\n",
    "* The agent starts in state **S** (see table below)\n",
    "* The actions possible are **N** (north), **S** (south), **E** (east), and **W** west. \n",
    "* The transition probabilities in each box are deterministic (for example P(s'|s,N)=1 if s' north of s). Note, however, that you cannot move outside the grid, thus all actions are not available in every box.\n",
    "* When reaching **F**, the game ends (absorbing state).\n",
    "* The numbers in the boxes represent the rewards you receive when moving into that box. \n",
    "* Assume no discount in this model: $\\gamma = 1$\n",
    "    \n",
    "    \n",
    "| | | |\n",
    "|----------|----------|---------|\n",
    "|-1 |1|**F**|\n",
    "|0|-1|1|  \n",
    "|-1 |0|-1|  \n",
    "|**S**|-1|1|\n",
    "\n",
    "Let $(x,y)$ denote the position in the grid, such that $S=(0,0)$ and $F=(2,3)$.\n",
    "\n",
    "**1a)** What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. E.g. NESW will make a circle.\n",
    "\n",
    "**Ans:** One of the optimal paths is EENNN. The optimal path is not unique. Another optimal path can be EENNWNE.\n",
    "\n",
    "**1b)** What is the optimal policy (i.e. the optimal action in each state)? It is helpful if you draw the arrows/letters in the grid.\n",
    "\n",
    "**Ans:** The optimal action is indicated by the direction of the arrow next to each state.\n",
    "\n",
    "| | | |\n",
    "|----------|----------|---------|\n",
    "|-1 >|1 >|**F**|\n",
    "|0 >|-1 >|1 ^|  \n",
    "|-1 >|0 >|-1 ^|  \n",
    "|**S** >|-1 >|1 ^|\n",
    "\n",
    "**1c)** What is expected total reward for the policy in 1a)?\n",
    "\n",
    "**Ans:** The total reward is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNkIk-k7qItT"
   },
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3vIdFpuqItU"
   },
   "source": [
    "## Question 2\n",
    "\n",
    "**2a)** Code the value iteration algorithm just described here, and show the converging optimal value function and the optimal policy for the above 3x3 grid.\n",
    "\n",
    "**Ans:** Code and converging values below.\n",
    "\n",
    "**2b)** Explain why the result of 2a) does not depend on the initial value $V_0$.\n",
    "\n",
    "**Ans:** It does not depend on the initial value because we stop iterating only when the value function does not change much (Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V^*$ in the right-hand side (RHS) of the Bellman equation should result in the same $V^*$ on the left-hand side (LHS)). So it's more about the changes/difference in value function based on the rewards than the initial value function $V_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value function below\n",
      "[44.791763498219886, 51.12689178471024, 44.791763498219886]\n",
      "[51.12689178471024, 47.23078788846379, 51.12689178471024]\n",
      "[44.791763498219886, 51.12689178471024, 44.791763498219886]\n",
      "\n",
      "Optimal policy below\n",
      "['S', 'S', 'S']\n",
      "['E', 'N', 'W']\n",
      "['N', 'N', 'N']\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "flag = True\n",
    "reward = [[0, 0, 0],[0, 10, 0], [0, 0, 0]]\n",
    "value = [[0, 0, 0],[0, 0, 0], [0, 0, 0]]\n",
    "steps = {'N': 0, 'S': 0, 'E': 0, 'W': 0}\n",
    "policy = [['', '', ''],['', '', ''], ['', '', '']]\n",
    "\n",
    "while flag:\n",
    "    temp = deepcopy(value)\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            for step in steps:\n",
    "                if step == 'N':\n",
    "                    if i == 0:\n",
    "                        steps[step] = 0\n",
    "                    else:\n",
    "                        steps[step] = (0.8*(reward[i-1][j] + gamma*value[i-1][j])) + (0.2*(reward[i][j] + gamma*value[i][j]))\n",
    "                if step == 'S':\n",
    "                    if i == 2:\n",
    "                        steps[step] = 0\n",
    "                    else:\n",
    "                        steps[step] = (0.8*(reward[i+1][j] + gamma*value[i+1][j])) + (0.2*(reward[i][j] + gamma*value[i][j]))\n",
    "                if step == 'E':\n",
    "                    if j == 2:\n",
    "                        steps[step] = 0\n",
    "                    else:\n",
    "                        steps[step] = (0.8*(reward[i][j+1] + gamma*value[i][j+1])) + (0.2*(reward[i][j] + gamma*value[i][j]))\n",
    "                if step == 'W':\n",
    "                    if j == 0:\n",
    "                        steps[step] = 0\n",
    "                    else:\n",
    "                        steps[step] = (0.8*(reward[i][j-1] + gamma*value[i][j-1])) + (0.2*(reward[i][j] + gamma*value[i][j]))\n",
    "            temp[i][j] = max(steps.values())\n",
    "            next_step = [key for key in steps if steps[key] == temp[i][j]]\n",
    "            policy[i][j] = next_step[0]\n",
    "    \n",
    "    stop = True\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if abs(temp[i][j] - value[i][j]) > epsilon:\n",
    "                stop = False\n",
    "    if stop:\n",
    "        flag = False\n",
    "    \n",
    "    value = temp\n",
    "\n",
    "print('Optimal value function below')\n",
    "for i in range(3):\n",
    "    print(value[i])\n",
    "\n",
    "print('\\nOptimal policy below')\n",
    "for i in range(3):\n",
    "    print(policy[i])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9tL23YlqItU"
   },
   "source": [
    "## Reinforcement Learning (RL)\n",
    "\n",
    "## Question 3\n",
    "You are to first familiarize with the framework of [the OpenAI environments](https://www.gymlibrary.dev/), and then implement the Q-learning algorithm for the <code>NChain-v0</code> enviroment depicted above, using default parameters and a learning rate of $\\gamma=0.95$. Report the final $Q^*$ table after convergence of the algorithm. For an example on how to do this, you can refer to the Q-learning of the **Frozen lake environment** (<code>q_learning_frozen_lake.ipynb</code>), uploaded on Canvas. Hint: start with a small learning rate.\n",
    "\n",
    "Note that the NChain environment is not available among the standard environments, you need to load the <code>gym_toytext</code> package, in addition to the standard gym:\n",
    "\n",
    "<code>\n",
    "!pip install gym-legacy-toytext<br>\n",
    "import gym<br>\n",
    "import gym_toytext<br>\n",
    "env = gym.make(\"NChain-v0\")<br>\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import gym_toytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"NChain-v0\")\n",
    "env.reset()\n",
    "\n",
    "num_episodes = 25000\n",
    "gamma = 0.95\n",
    "learning_rate = 0.1\n",
    "epsilon = 0.5\n",
    "\n",
    "Q = np.zeros([5, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_episodes):\n",
    "\tstate = env.reset()\n",
    "\tdone = False\n",
    "\twhile done == False:\n",
    "\t\tif random.uniform(0, 1) < epsilon:\n",
    "\t\t\taction = env.action_space.sample()\n",
    "\t\telse:\n",
    "\t\t\taction = np.argmax(Q[state,:])\n",
    "\t\tnew_state, reward, done, info = env.step(action)\n",
    "\t\tupdate = reward + (gamma*np.max(Q[new_state,:])) - Q[state, action]\n",
    "\t\tQ[state,action] += learning_rate*update \n",
    "\t\tstate = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Action A   Action B\n",
      "1  60.813210  59.911587\n",
      "2  64.371282  60.617006\n",
      "3  69.599451  62.175039\n",
      "4  73.659266  63.484101\n",
      "5  79.573522  63.641493\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(Q, columns=['Action A', 'Action B'])\n",
    "df.index = np.arange(1, len(df) + 1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfKSybVI-UN1"
   },
   "source": [
    "## Question 4\n",
    "\n",
    "**4a)** What is the importance of exploration in RL? Explain with an example.\n",
    "\n",
    "**Ans:** Exploration is the only way for reinforcement algorithms to learn and get more information about unknown parts of the environment. If there was no exploration then the algorithm will only optimize on the data it already has but with exploration it can learn more information and get better results. An example showing the importance of exploration is when one wants to know the current state of affairs in the world it might be good to read the news from new sources instead of just reading the news from the sources that one normally uses.\n",
    "\n",
    "**4b)** Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification.\n",
    "\n",
    "**Ans:** In supervised learning decisions are made using predefined set of data which hopefully represent a full picture of the world, but in reinforcement learning, decisions are made without knowing the entire picture of the world. Learning is done based on the result of each decision/exploration taken and its used in finding the optimal decisions to be made in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1iFSvirqItV"
   },
   "source": [
    "## Question 5\n",
    "\n",
    "**5a)** Give a summary of how a decision tree works and how it extends to random forests.\n",
    "\n",
    "**Ans:** A decision tree helps with classification of data. The tree is built using the trends and correlations between the features and classes. And when it's time to predict, the new data is tested on the decision tree by starting from the root of the tree and taking the path based on the data and the various questions being asked in each node of the tree and finding the next node based the answer to the question. Random forests is a set of decision trees on the same set of data but multiple decision trees are created by selecting only a subset of the features (chosen randomly) and building decison trees. \n",
    "\n",
    "**5b)** State at least one advantage and one drawback with using random forests over decision trees.\n",
    "\n",
    "**Ans:** Random forests help overcome the overfitting problem of decison trees since only a subset of the features are taken for each decision tree of the random forrest and thus reducing the overfitting. Random forests can also handle missing data for certain number of features. Drawback of random forest is that it can be slow especially when the number of the features is quite high. This is because, during prediction each decison tree must do a prediction based on the data and this needs to be cumulated before making the final decision for classification."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
